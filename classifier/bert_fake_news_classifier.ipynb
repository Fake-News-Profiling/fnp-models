{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_hub import KerasLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from official.nlp import optimization\n",
    "\n",
    "from official.nlp.bert.tokenization import FullTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractBertTokenizer(ABC):\n",
    "    \"\"\" Abstract BERT Tokenizer\"\"\"\n",
    "    label_pattern = None\n",
    "    \n",
    "    def __init__(self, encoder, bert_input_size):\n",
    "        \"\"\" Create the BERT encoder and tokenizer \"\"\"\n",
    "        self.bert_input_size = bert_input_size\n",
    "        self.tokenizer = FullTokenizer(\n",
    "            encoder.resolved_object.vocab_file.asset_path.numpy(), \n",
    "            do_lower_case=encoder.resolved_object.do_lower_case.numpy()\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def tokenize_input(self, x):\n",
    "        \"\"\" Tokenize input data \"\"\"\n",
    "        return\n",
    "\n",
    "    def tokenize_labels(self, y):\n",
    "        \"\"\" Tokenize input data labels \"\"\"\n",
    "        if self.label_pattern is not None:\n",
    "            labels = [int(v) for v,n in zip(y, self.label_pattern) for i in range(n)]\n",
    "            return tf.convert_to_tensor(labels, tf.int32)\n",
    "        else:\n",
    "            raise Exception(\"Must tokenize the input first\")\n",
    "    \n",
    "    def _format_bert_tokens(self, ragged_word_ids):\n",
    "        \"\"\" Create, format and pad BERT's input tensors \"\"\"\n",
    "        # Generate mask, and pad word_ids and mask\n",
    "        mask = tf.ones_like(ragged_word_ids).to_tensor()\n",
    "        word_ids = ragged_word_ids.to_tensor()\n",
    "        padding = tf.constant([[0, 0], [0, (self.bert_input_size - mask.shape[1])]])\n",
    "        word_ids = tf.pad(word_ids, padding, \"CONSTANT\")\n",
    "        mask = tf.pad(mask, padding, \"CONSTANT\")\n",
    "        type_ids = tf.zeros_like(mask)\n",
    "        \n",
    "        return {\n",
    "            'input_word_ids': word_ids,\n",
    "            'input_mask': mask,\n",
    "            'input_type_ids': type_ids,\n",
    "        }\n",
    "    \n",
    "    def _format_bert_word_piece_input(self, word_piece_tokens):\n",
    "        word_piece_tokens.insert(0, '[CLS]')\n",
    "        word_piece_tokens.append('[SEP]')\n",
    "        return self.tokenizer.convert_tokens_to_ids(word_piece_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIndividualTweetTokenizer(AbstractBertTokenizer):\n",
    "    \"\"\" BERT tokenizer which tokenizes historical tweet data as individual tweets \"\"\"\n",
    "    \n",
    "    def tokenize_input(self, X):\n",
    "        \"\"\" Tokenize input data \"\"\"\n",
    "        tokenized_tweets = [\n",
    "            self._tokenize_single_tweet(tweet) for tweet_feed in X for tweet in tweet_feed\n",
    "        ]\n",
    "        self.label_pattern = [len(tweet_feed) for tweet_feed in X]\n",
    "        word_ids = tf.ragged.constant(tokenized_tweets)\n",
    "        return self._format_bert_tokens(word_ids)\n",
    "\n",
    "    def _tokenize_single_tweet(self, tweet):\n",
    "        \"\"\" Tokenize a single tweet, truncating its tokens to bert_input_size \"\"\"\n",
    "        tokens = self.tokenizer.tokenize(tweet)[:self.bert_input_size-2]\n",
    "        return self._format_bert_word_piece_input(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTweetFeedTokenizer(AbstractBertTokenizer):\n",
    "    \"\"\" BERT tokenizer which tokenizes historical tweet data as tweet feed chunks \"\"\"\n",
    "    \n",
    "    def tokenize_input(self, X, overlap=50):\n",
    "        \"\"\" Tokenize input data \"\"\"\n",
    "        tokenized_tweet_feeds = [\n",
    "            self._tokenize_tweet_feed(\" \".join(tweet_feed), overlap) for tweet_feed in X\n",
    "        ]\n",
    "        self.label_pattern = [len(tweet_feed) for tweet_feed in tokenized_tweet_feeds]\n",
    "        flattened_feeds = [chunk for feed in tokenized_tweet_feeds for chunk in feed]\n",
    "        word_ids = tf.ragged.constant(flattened_feeds)\n",
    "        return self._format_bert_tokens(word_ids)\n",
    "    \n",
    "    def _tokenize_tweet_feed(self, tweet_feed, overlap):\n",
    "        \"\"\" Tokenize an entire tweet feed into chunks \"\"\"\n",
    "        feed_tokens = self.tokenizer.tokenize(tweet_feed)\n",
    "        tokens = [\n",
    "            feed_tokens[i:i+self.bert_input_size-2] \n",
    "            for i in range(0, len(feed_tokens), self.bert_input_size-overlap)\n",
    "        ]\n",
    "\n",
    "        return list(map(self._format_bert_word_piece_input, tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_bert_model(encoder, input_size):\n",
    "    # Create BERT input layers\n",
    "    def input_layer(input_name):\n",
    "        return Input(shape=(input_size,), dtype=tf.int32, name=input_name)\n",
    "\n",
    "    inputs = {\n",
    "        'input_word_ids': input_layer(\"inputs/input_word_ids\"),\n",
    "        'input_mask': input_layer(\"inputs/input_mask\"),\n",
    "        'input_type_ids': input_layer(\"inputs/input_type_ids\"),\n",
    "    }\n",
    "\n",
    "    # BERT's output\n",
    "    return inputs, encoder(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT model with a Dense sigmoid output layer\n",
    "def dense_bert_model(encoder, input_size):\n",
    "    inputs, bert_output = base_bert_model(encoder, input_size)\n",
    "\n",
    "    # Dense layer output\n",
    "    dense_output = Dense(1, activation='sigmoid')(bert_output['pooled_output'])\n",
    "\n",
    "    # Create the Keras model and compile\n",
    "    return Model(inputs, dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT model with an LSTM output layer\n",
    "def create_bert_model_lstm(encoder, input_size, **kwargs):\n",
    "    bert_pooled_output = base_bert_model(encoder, input_size)['pooled_output']\n",
    "\n",
    "    # LSTM layer output\n",
    "    dense_output = LSTM(1, **kwargs)(bert_pooled_output)\n",
    "\n",
    "    # Create the Keras model and compile\n",
    "    return Model(inputs, dense_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelEvalHandler(ABC):\n",
    "    def __init__(self, bert_url: str, bert_input_size: int, bert_tokenizer_class: AbstractBertTokenizer.__class__,\n",
    "                bert_model: Callable):\n",
    "        self.encoder = KerasLayer(bert_url, trainable=True)\n",
    "        self.tokenizer = bert_tokenizer_class(self.encoder, bert_input_size)\n",
    "        self.bert = bert_model(self.encoder, bert_input_size)\n",
    "    \n",
    "    def train_bert(self, X, y, batch_size, epochs, X_validation, y_validation, optimizer_name, lr, checkpoint_path=None, \n",
    "                   tensorboard_path=None):\n",
    "        # Tokenize input data\n",
    "        X = self.tokenizer.tokenize_input(X)\n",
    "        y = self.tokenizer.tokenize_labels(y)\n",
    "        X_validation = self.tokenizer.tokenize_input(X_validation)\n",
    "        y_validation = self.tokenizer.tokenize_labels(y_validation)\n",
    "        \n",
    "        # Setup callbacks\n",
    "        callbacks = []\n",
    "        if checkpoint_path is not None:\n",
    "            callbacks.append(ModelCheckpoint(\n",
    "                filepath=checkpoint_path, \n",
    "                save_weights_only=True,\n",
    "                save_best_only=True,\n",
    "                verbose=1,\n",
    "            ))\n",
    "        \n",
    "        if tensorboard_path is not None:\n",
    "            callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=tensorboard_path))\n",
    "        \n",
    "        # Setup optimizer\n",
    "        if optimizer_name == 'adam':\n",
    "            optimizer = tf.keras.optimizers.Adam(lr)\n",
    "        elif optimizer_name == 'adamw':\n",
    "            total_training_steps = epochs * len(X['input_word_ids']) / batch_size\n",
    "            warmup_steps = int(0.1 * total_training_steps)\n",
    "            optimizer = optimization.create_optimizer(\n",
    "                init_lr=lr,\n",
    "                num_train_steps=total_training_steps,\n",
    "                num_warmup_steps=warmup_steps,\n",
    "                optimizer_type='adamw'\n",
    "            )\n",
    "        \n",
    "        # Compile and train BERT\n",
    "        self.bert.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "            metrics=tf.metrics.BinaryAccuracy(),\n",
    "        )\n",
    "        \n",
    "        self.train_history = self.bert.fit(\n",
    "            x=X,\n",
    "            y=y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_validation, y_validation),\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        return self.train_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
