{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch input and parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code fetches the datasets, extracts the xml for each author, and returns two Python dictionaries:\n",
    "* author_tweets - maps each authors id to a list of their tweets.\n",
    "* author_truths - maps each authors id to a truth value 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __parse_author_tweets(xml_filepaths):\n",
    "    \"\"\"Returns a dictionary of authors to a list of their tweets\"\"\"\n",
    "    author_tweets = {}\n",
    "    for filepath in xml_filepaths:\n",
    "        xml_tree = ET.parse(filepath)\n",
    "        documents = xml_tree.getroot()[0]\n",
    "        file_path_components = filepath.split(\"\\\\\")\n",
    "        file = file_path_components[len(file_path_components)-1]\n",
    "        \n",
    "        author = file[0:len(file)-4]\n",
    "        tweets = [document.text for document in documents]\n",
    "        author_tweets[author] = tweets\n",
    "    \n",
    "    return author_tweets\n",
    "\n",
    "def __parse_author_truths(truth_filepath):\n",
    "    \"\"\"Returns a dictionary of authors to their truth values 1/0\"\"\"\n",
    "    author_truths = {}\n",
    "    with open(truth_filepath, 'r') as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            author, truth = line.rstrip().split(\":::\")\n",
    "            author_truths[author] = truth\n",
    "            line = fp.readline()\n",
    "    \n",
    "    return author_truths\n",
    "\n",
    "def __filter_files(datasets_path, files, file_type):\n",
    "    filtered = filter(lambda f: f.endswith(file_type), files)\n",
    "    return list(map(lambda f: os.path.join(datasets_path, f), filtered))\n",
    "\n",
    "def parse_datasets_language(datasets_path, language):\n",
    "    \"\"\"\n",
    "    Keyword arguments:\n",
    "    datasets_path -- path to the datasets directory\n",
    "    language -- the language dataset to use, either \"en\" or \"es\"\n",
    "    \n",
    "    Returns pandas DataFrame, where each row contains an author id, truth value, and tweets 1 to 100.\n",
    "    \"\"\"\n",
    "    language_path = os.path.join(datasets_path, language)\n",
    "    \n",
    "    # Get each file in the directory and filter by .xml and .txt extensions.\n",
    "    files = os.listdir(language_path)\n",
    "    xml_filepaths = __filter_files(language_path, files, \".xml\")\n",
    "    truth_filepath = __filter_files(language_path, files, \".txt\")[0]\n",
    "    \n",
    "    # Parse the files.\n",
    "    author_tweets = __parse_author_tweets(xml_filepaths)\n",
    "    author_truths = __parse_author_truths(truth_filepath)\n",
    "    \n",
    "    # Convert to a pandas DataFrame\n",
    "    data = []\n",
    "    for key, value in author_tweets.items():\n",
    "        d = {}\n",
    "        d[\"author_id\"] = key\n",
    "        d[\"truth_value\"] = author_truths[key]\n",
    "        for i, tweet in enumerate(value, start=1):\n",
    "            d[\"tweet_\"+str(i)] = tweet\n",
    "        \n",
    "        data.append(d)\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
