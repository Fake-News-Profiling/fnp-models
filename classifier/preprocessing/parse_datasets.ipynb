{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch input and parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Tweet:\n",
    "    \"\"\" Class to hold the contents of a Tweet \"\"\"\n",
    "    username: str\n",
    "    text: str\n",
    "    id: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_author_tweets(xml_filepaths):\n",
    "    \"\"\"Returns a dictionary of authors to a list of their tweets\"\"\"\n",
    "    author_tweets = {}\n",
    "    for filepath in xml_filepaths:\n",
    "        xml_tree = ET.parse(filepath)\n",
    "        documents = xml_tree.getroot()[0]\n",
    "        file_path_components = filepath.split(\"\\\\\")\n",
    "        file = file_path_components[len(file_path_components)-1]\n",
    "        \n",
    "        author = file[0:len(file)-4]\n",
    "        tweets = [document.text for document in documents]\n",
    "        author_tweets[author] = tweets\n",
    "    \n",
    "    return author_tweets\n",
    "\n",
    "def _parse_author_truths(truth_filepath):\n",
    "    \"\"\"Returns a dictionary of authors to their truth values 1/0\"\"\"\n",
    "    author_truths = {}\n",
    "    with open(truth_filepath, 'r') as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            author, truth = line.rstrip().split(\":::\")\n",
    "            author_truths[author] = truth\n",
    "            line = fp.readline()\n",
    "    \n",
    "    return author_truths\n",
    "\n",
    "def _filter_files(datasets_path, files, file_type):\n",
    "    filtered = filter(lambda f: f.endswith(file_type), files)\n",
    "    return list(map(lambda f: os.path.join(datasets_path, f), filtered))\n",
    "\n",
    "def parse_dataset(datasets_path, language, to_pandas=False):\n",
    "    \"\"\"\n",
    "    Keyword arguments:\n",
    "    datasets_path -- path to the datasets directory\n",
    "    language -- the language dataset to use, either \"en\" or \"es\"\n",
    "    \n",
    "    If to_pandas=True then returns pandas DataFrame, where each row contains an author id, truth value, and tweets 1 to 100.\n",
    "    Else returns an array of tweet feeds\n",
    "    \"\"\"\n",
    "    language_path = os.path.join(datasets_path, language)\n",
    "    \n",
    "    # Get each file in the directory and filter by .xml and .txt extensions.\n",
    "    files = os.listdir(language_path)\n",
    "    xml_filepaths = _filter_files(language_path, files, \".xml\")\n",
    "    truth_filepath = _filter_files(language_path, files, \".txt\")[0]\n",
    "    \n",
    "    # Parse the files.\n",
    "    author_tweets = _parse_author_tweets(xml_filepaths)\n",
    "    author_truths = _parse_author_truths(truth_filepath)\n",
    "    \n",
    "    if to_pandas:\n",
    "        # Convert to a pandas DataFrame\n",
    "        data = []\n",
    "        for key, value in author_tweets.items():\n",
    "            d = {}\n",
    "            d[\"author_id\"] = key\n",
    "            d[\"truth_value\"] = author_truths[key]\n",
    "            for i, tweet in enumerate(value, start=1):\n",
    "                d[\"tweet_\"+str(i)] = tweet\n",
    "\n",
    "            data.append(d)\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        # Convert to Tweet data objects\n",
    "        tweet_data = []\n",
    "        label_data = []\n",
    "        for author_id, tweet_feed in author_tweets.items():\n",
    "            tweet_data.append([Tweet(author_id, tweet, i) for i, tweet in enumerate(tweet_feed, start=1)])\n",
    "            label_data.append(author_truths[author_id])\n",
    "            \n",
    "        return np.asarray(tweet_data), np.asarray(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoding(Enum):\n",
    "    individual = 0\n",
    "    feed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(tweet_data, label_data, test_size=0.15, val_size=0.15):\n",
    "    \"\"\" Takes a tweet feed dataset \"\"\"\n",
    "    tweet_train, tweet_other, label_train, label_other = train_test_split(\n",
    "        tweet_data, label_data, test_size=(test_size + val_size),\n",
    "        random_state=0,\n",
    "        shuffle=True,\n",
    "        stratify=label_data,\n",
    "    )\n",
    "    tweet_val, tweet_test, label_val, label_test = train_test_split(\n",
    "        tweet_other, label_other, test_size=(test_size / (test_size + val_size)),\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "        stratify=label_other,\n",
    "    )\n",
    "\n",
    "    return (tweet_train, label_train, tweet_val, label_val, tweet_test, label_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
