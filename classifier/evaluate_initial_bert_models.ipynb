{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "preprocessing_path = 'C:\\\\Users\\\\joshh\\\\Desktop\\\\Uni\\\\Soton Year 3\\\\COMP3200\\\\fake-news-profiling\\\\classifier\\\\preprocessing'\n",
    "if preprocessing_path not in sys.path:\n",
    "    sys.path.insert(1, preprocessing_path)\n",
    "\n",
    "notif_path = 'C:\\\\Users\\\\joshh\\\\Desktop\\\\Uni\\\\Soton Year 3\\\\COMP3200\\\\fake-news-profiling\\\\classifier\\\\notifications'\n",
    "if notif_path not in sys.path:\n",
    "    sys.path.insert(1, notif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import ipynb.fs.full.parse_datasets as datasets\n",
    "import ipynb.fs.full.preprocessing as pp\n",
    "import ipynb.fs.full.bert_fake_news_classifier as bclf\n",
    "from ipynb.fs.full.notif_email import send_email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data, label_data = datasets.parse_dataset(\"datasets\", \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "tweet_preprocessor = pp.TweetPreprocessor(\n",
    "    preprocess_funcs = [\n",
    "        pp.tag_indicators,\n",
    "        pp.replace_xml_and_html,\n",
    "        pp.replace_emojis,\n",
    "        pp.remove_punctuation,\n",
    "        pp.replace_tags,\n",
    "        pp.remove_hashtag_chars,\n",
    "        pp.replace_accented_chars,\n",
    "        pp.tag_numbers,\n",
    "        pp.remove_stopwords,\n",
    "        pp.remove_extra_spacing,\n",
    "    ])\n",
    "tweet_preprocessor.preprocess(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_individual = tweet_preprocessor.get_individual_tweets_dataset()\n",
    "tweet_data_feed = tweet_preprocessor.get_tweet_feed_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "(tweet_train, label_train, \n",
    " tweet_val, label_val, \n",
    " tweet_test, label_test) = datasets.split_dataset(tweet_data_individual, label_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Individual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_bert_url = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1\"\n",
    "bert_encoder_individual = hub.KerasLayer(\n",
    "    small_bert_url, \n",
    "    trainable=True,\n",
    ")\n",
    "\n",
    "bert_input_size_individual = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_tokenizer = bclf.BertIndividualTweetTokenizer(bert_encoder_individual, bert_input_size_individual)\n",
    "bert_model_individual = bclf.create_bert_model(bert_encoder_individual, bert_input_size_individual)\n",
    "bert_model_individual.compile(Adam(lr=1e-5), 'binary_crossentropy', ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a checkpoint for training\n",
    "checkpoint_path_individual = \"training/bert_training_individual_1/cp.ckpt\"\n",
    "\n",
    "bert_checkpoint_callback_individual = ModelCheckpoint(\n",
    "    filepath=checkpoint_path_individual,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweet_individual_train = individual_tokenizer.tokenize_input(tweet_train)\n",
    "label_individual_train = individual_tokenizer.tokenize_labels(label_train)\n",
    "tweet_individual_val = individual_tokenizer.tokenize_input(tweet_val)\n",
    "label_individual_val = individual_tokenizer.tokenize_labels(label_val)\n",
    "tweet_individual_test = individual_tokenizer.tokenize_input(tweet_test)\n",
    "label_individual_test = individual_tokenizer.tokenize_labels(label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding optimal hyper parameters (batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "329/329 [==============================] - 81s 246ms/step - loss: 0.6682 - accuracy: 0.5874\n",
      "Epoch 2/10\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.6340 - accuracy: 0.6392\n",
      "Epoch 3/10\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.6187 - accuracy: 0.6549\n",
      "Epoch 4/10\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5999 - accuracy: 0.6768\n",
      "Epoch 5/10\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5834 - accuracy: 0.6926\n",
      "Epoch 6/10\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5652 - accuracy: 0.7097\n",
      "Epoch 7/10\n",
      "329/329 [==============================] - 81s 246ms/step - loss: 0.5473 - accuracy: 0.7217\n",
      "Epoch 8/10\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5304 - accuracy: 0.7325\n",
      "Epoch 9/10\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5142 - accuracy: 0.7476\n",
      "Epoch 10/10\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4966 - accuracy: 0.7599\n",
      "141/141 [==============================] - 6s 42ms/step - loss: 0.7258 - accuracy: 0.6438\n",
      "{'batch_size': 64, 'epochs': 10, 'loss': 0.725799024105072, 'accuracy': 0.6437777876853943}\n",
      "Epoch 1/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.6675 - accuracy: 0.5849\n",
      "Epoch 2/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.6373 - accuracy: 0.6348\n",
      "Epoch 3/50\n",
      "329/329 [==============================] - 81s 248ms/step - loss: 0.6210 - accuracy: 0.6556\n",
      "Epoch 4/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.6013 - accuracy: 0.6748\n",
      "Epoch 5/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5870 - accuracy: 0.6868\n",
      "Epoch 6/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5705 - accuracy: 0.7022\n",
      "Epoch 7/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5506 - accuracy: 0.7178\n",
      "Epoch 8/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5337 - accuracy: 0.7310\n",
      "Epoch 9/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5162 - accuracy: 0.7440\n",
      "Epoch 10/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5007 - accuracy: 0.7552\n",
      "Epoch 11/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4855 - accuracy: 0.7650\n",
      "Epoch 12/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4693 - accuracy: 0.7762\n",
      "Epoch 13/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4535 - accuracy: 0.7846\n",
      "Epoch 14/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4432 - accuracy: 0.7926\n",
      "Epoch 15/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4250 - accuracy: 0.8017\n",
      "Epoch 16/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4130 - accuracy: 0.8084\n",
      "Epoch 17/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3982 - accuracy: 0.8176\n",
      "Epoch 18/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3882 - accuracy: 0.8230\n",
      "Epoch 19/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3713 - accuracy: 0.8344\n",
      "Epoch 20/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3604 - accuracy: 0.8385\n",
      "Epoch 21/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3503 - accuracy: 0.8455\n",
      "Epoch 22/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3419 - accuracy: 0.8488\n",
      "Epoch 23/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3281 - accuracy: 0.8574\n",
      "Epoch 24/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3175 - accuracy: 0.8609\n",
      "Epoch 25/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3039 - accuracy: 0.8677\n",
      "Epoch 26/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2986 - accuracy: 0.8721\n",
      "Epoch 27/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2861 - accuracy: 0.8782\n",
      "Epoch 28/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2801 - accuracy: 0.8797\n",
      "Epoch 29/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2720 - accuracy: 0.8842\n",
      "Epoch 30/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2623 - accuracy: 0.8892\n",
      "Epoch 31/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2534 - accuracy: 0.8934\n",
      "Epoch 32/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2464 - accuracy: 0.8994\n",
      "Epoch 33/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2390 - accuracy: 0.9009\n",
      "Epoch 34/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2288 - accuracy: 0.9050\n",
      "Epoch 35/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2200 - accuracy: 0.9079\n",
      "Epoch 36/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2117 - accuracy: 0.9137\n",
      "Epoch 37/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2072 - accuracy: 0.9151\n",
      "Epoch 38/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2022 - accuracy: 0.9154\n",
      "Epoch 39/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1959 - accuracy: 0.9206\n",
      "Epoch 40/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1913 - accuracy: 0.9209\n",
      "Epoch 41/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1832 - accuracy: 0.9259\n",
      "Epoch 42/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1751 - accuracy: 0.9272\n",
      "Epoch 43/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1680 - accuracy: 0.9310\n",
      "Epoch 44/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1686 - accuracy: 0.9327\n",
      "Epoch 45/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1600 - accuracy: 0.9360\n",
      "Epoch 46/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1538 - accuracy: 0.9382\n",
      "Epoch 47/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1507 - accuracy: 0.9387\n",
      "Epoch 48/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1504 - accuracy: 0.9412\n",
      "Epoch 49/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1421 - accuracy: 0.9447\n",
      "Epoch 50/50\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1409 - accuracy: 0.9453\n",
      "141/141 [==============================] - 6s 42ms/step - loss: 1.4172 - accuracy: 0.6007\n",
      "{'batch_size': 64, 'epochs': 50, 'loss': 1.4171788692474365, 'accuracy': 0.6006666421890259}\n",
      "Epoch 1/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.6698 - accuracy: 0.5900\n",
      "Epoch 2/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.6373 - accuracy: 0.6343\n",
      "Epoch 3/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.6187 - accuracy: 0.6573\n",
      "Epoch 4/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.6033 - accuracy: 0.6720\n",
      "Epoch 5/100\n",
      "329/329 [==============================] - 82s 248ms/step - loss: 0.5861 - accuracy: 0.6903\n",
      "Epoch 6/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5716 - accuracy: 0.7026\n",
      "Epoch 7/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5514 - accuracy: 0.7180\n",
      "Epoch 8/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5372 - accuracy: 0.7263\n",
      "Epoch 9/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5229 - accuracy: 0.7405\n",
      "Epoch 10/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.5041 - accuracy: 0.7526\n",
      "Epoch 11/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4880 - accuracy: 0.7659\n",
      "Epoch 12/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4755 - accuracy: 0.7714\n",
      "Epoch 13/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4594 - accuracy: 0.7842\n",
      "Epoch 14/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4424 - accuracy: 0.7936\n",
      "Epoch 15/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4319 - accuracy: 0.8002\n",
      "Epoch 16/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4131 - accuracy: 0.8091\n",
      "Epoch 17/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.4057 - accuracy: 0.8139\n",
      "Epoch 18/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3897 - accuracy: 0.8245\n",
      "Epoch 19/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3792 - accuracy: 0.8281\n",
      "Epoch 20/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3696 - accuracy: 0.8358\n",
      "Epoch 21/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3486 - accuracy: 0.8451\n",
      "Epoch 22/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3392 - accuracy: 0.8492\n",
      "Epoch 23/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3348 - accuracy: 0.8530\n",
      "Epoch 24/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3168 - accuracy: 0.8631\n",
      "Epoch 25/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.3130 - accuracy: 0.8646\n",
      "Epoch 26/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2983 - accuracy: 0.8709\n",
      "Epoch 27/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2949 - accuracy: 0.8744\n",
      "Epoch 28/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2815 - accuracy: 0.8796\n",
      "Epoch 29/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2729 - accuracy: 0.8846\n",
      "Epoch 30/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2652 - accuracy: 0.8871\n",
      "Epoch 31/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2554 - accuracy: 0.8932\n",
      "Epoch 32/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2456 - accuracy: 0.8962\n",
      "Epoch 33/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2393 - accuracy: 0.8998\n",
      "Epoch 34/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2316 - accuracy: 0.9025\n",
      "Epoch 35/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2296 - accuracy: 0.9048\n",
      "Epoch 36/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2193 - accuracy: 0.9088\n",
      "Epoch 37/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2104 - accuracy: 0.9136\n",
      "Epoch 38/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2030 - accuracy: 0.9153\n",
      "Epoch 39/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.2011 - accuracy: 0.9170\n",
      "Epoch 40/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1846 - accuracy: 0.9250\n",
      "Epoch 41/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1850 - accuracy: 0.9237\n",
      "Epoch 42/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1820 - accuracy: 0.9260\n",
      "Epoch 43/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1770 - accuracy: 0.9267\n",
      "Epoch 44/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1655 - accuracy: 0.9324\n",
      "Epoch 45/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1608 - accuracy: 0.9365\n",
      "Epoch 46/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1557 - accuracy: 0.9387\n",
      "Epoch 47/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1529 - accuracy: 0.9383\n",
      "Epoch 48/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1469 - accuracy: 0.9421\n",
      "Epoch 49/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1424 - accuracy: 0.9431\n",
      "Epoch 50/100\n",
      "329/329 [==============================] - 82s 248ms/step - loss: 0.1413 - accuracy: 0.9450\n",
      "Epoch 51/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1409 - accuracy: 0.9440\n",
      "Epoch 52/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1323 - accuracy: 0.9471\n",
      "Epoch 53/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1336 - accuracy: 0.9481\n",
      "Epoch 54/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1280 - accuracy: 0.9491\n",
      "Epoch 55/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1281 - accuracy: 0.9483\n",
      "Epoch 56/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1231 - accuracy: 0.9516\n",
      "Epoch 57/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1142 - accuracy: 0.9564\n",
      "Epoch 58/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1152 - accuracy: 0.9554\n",
      "Epoch 59/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1135 - accuracy: 0.9556\n",
      "Epoch 60/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1098 - accuracy: 0.9569\n",
      "Epoch 61/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1027 - accuracy: 0.9608\n",
      "Epoch 62/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1016 - accuracy: 0.9603\n",
      "Epoch 63/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.1015 - accuracy: 0.9595\n",
      "Epoch 64/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0972 - accuracy: 0.9619\n",
      "Epoch 65/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0984 - accuracy: 0.9616\n",
      "Epoch 66/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0892 - accuracy: 0.9651\n",
      "Epoch 67/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0884 - accuracy: 0.9658\n",
      "Epoch 68/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0883 - accuracy: 0.9667\n",
      "Epoch 69/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0846 - accuracy: 0.9663\n",
      "Epoch 70/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0825 - accuracy: 0.9676\n",
      "Epoch 71/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0842 - accuracy: 0.9683\n",
      "Epoch 72/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0788 - accuracy: 0.9693\n",
      "Epoch 73/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0842 - accuracy: 0.9679\n",
      "Epoch 74/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0777 - accuracy: 0.9692\n",
      "Epoch 75/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0780 - accuracy: 0.9701\n",
      "Epoch 76/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0775 - accuracy: 0.9697\n",
      "Epoch 77/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0753 - accuracy: 0.9705\n",
      "Epoch 78/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0707 - accuracy: 0.9730\n",
      "Epoch 79/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0707 - accuracy: 0.9729\n",
      "Epoch 80/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0734 - accuracy: 0.9721\n",
      "Epoch 81/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0686 - accuracy: 0.9716\n",
      "Epoch 82/100\n",
      "329/329 [==============================] - 81s 248ms/step - loss: 0.0675 - accuracy: 0.9730\n",
      "Epoch 83/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0652 - accuracy: 0.9743\n",
      "Epoch 84/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0641 - accuracy: 0.9754\n",
      "Epoch 85/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0604 - accuracy: 0.9750\n",
      "Epoch 86/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0631 - accuracy: 0.9756\n",
      "Epoch 87/100\n",
      "329/329 [==============================] - 81s 248ms/step - loss: 0.0632 - accuracy: 0.9757\n",
      "Epoch 88/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0618 - accuracy: 0.9762\n",
      "Epoch 89/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0626 - accuracy: 0.9753\n",
      "Epoch 90/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0593 - accuracy: 0.9762\n",
      "Epoch 91/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0558 - accuracy: 0.9779\n",
      "Epoch 92/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0596 - accuracy: 0.9773\n",
      "Epoch 93/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0573 - accuracy: 0.9783\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0514 - accuracy: 0.9796\n",
      "Epoch 95/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0529 - accuracy: 0.9806\n",
      "Epoch 96/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0484 - accuracy: 0.9811\n",
      "Epoch 97/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0541 - accuracy: 0.9789\n",
      "Epoch 98/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0518 - accuracy: 0.9798\n",
      "Epoch 99/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0523 - accuracy: 0.9791\n",
      "Epoch 100/100\n",
      "329/329 [==============================] - 81s 247ms/step - loss: 0.0497 - accuracy: 0.9812\n",
      "141/141 [==============================] - 6s 42ms/step - loss: 2.0028 - accuracy: 0.6007\n",
      "{'batch_size': 64, 'epochs': 100, 'loss': 2.0028185844421387, 'accuracy': 0.6006666421890259}\n"
     ]
    }
   ],
   "source": [
    "results = [{'batch_size': 1, 'epochs': 10, 'loss': 1.2027349472045898, 'accuracy': 0.570888876914978}, {'batch_size': 8, 'epochs': 10, 'loss': 0.8909910321235657, 'accuracy': 0.5973333120346069}, {'batch_size': 8, 'epochs': 50, 'loss': 2.1527915000915527, 'accuracy': 0.5951111316680908}, {'batch_size': 8, 'epochs': 100, 'loss': 2.6716349124908447, 'accuracy': 0.5737777948379517}, {'batch_size': 32, 'epochs': 10, 'loss': 0.8014382719993591, 'accuracy': 0.5855555534362793}, {'batch_size': 32, 'epochs': 50, 'loss': 1.791918396949768, 'accuracy': 0.5933333039283752}, {'batch_size': 32, 'epochs': 100, 'loss': 2.2307729721069336, 'accuracy': 0.5973333120346069}]\n",
    "batch_sizes = [64]\n",
    "epochs = [10, 50, 100]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for epoch in epochs:\n",
    "\n",
    "        # Fit the model and then evaluate\n",
    "        with tf.device('gpu:0'):\n",
    "            bert_encoder_individual_test = hub.KerasLayer(\n",
    "                small_bert_url, \n",
    "                trainable=True,\n",
    "            )\n",
    "            bert_model_individual_test = clf.create_bert_model(bert_encoder_individual_test, bert_input_size_individual)\n",
    "            bert_model_individual_test.compile(Adam(lr=1e-5), 'binary_crossentropy', ['accuracy'])\n",
    "            \n",
    "            bert_model_individual_test.fit(\n",
    "                x=tweet_individual_train, \n",
    "                y=label_individual_train, \n",
    "                batch_size=batch_size, \n",
    "                epochs=epoch,\n",
    "            )\n",
    "            \n",
    "            evaluated_results = bert_model_individual_test.evaluate(tweet_individual_val, label_individual_val)\n",
    "            results.append({\n",
    "                'batch_size': batch_size, \n",
    "                'epochs': epoch, \n",
    "                'loss': evaluated_results[0], \n",
    "                'accuracy': evaluated_results[1]\n",
    "            })\n",
    "            print(results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'epochs': 10,\n",
       " 'loss': 0.725799024105072,\n",
       " 'accuracy': 0.6437777876853943}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result = max(results, key=lambda result: result['accuracy'])\n",
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_email(\n",
    "    f\"\"\"\n",
    "    Grid Search finished.\n",
    "    Best model: \n",
    "    > batch_size: {best_result['batch_size']}\n",
    "    > epochs: {best_result['epochs']}\n",
    "    > loss: {best_result['loss']}\n",
    "    > accuracy: {best_result['accuracy']}\n",
    "    \n",
    "    All results: {results}\n",
    "    \n",
    "    Now training model.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [{'batch_size': 1,\n",
    "  'epochs': 10,\n",
    "  'loss': 1.2027349472045898,\n",
    "  'accuracy': 0.570888876914978},\n",
    " {'batch_size': 8,\n",
    "  'epochs': 10,\n",
    "  'loss': 0.8909910321235657,\n",
    "  'accuracy': 0.5973333120346069},\n",
    " {'batch_size': 8,\n",
    "  'epochs': 50,\n",
    "  'loss': 2.1527915000915527,\n",
    "  'accuracy': 0.5951111316680908},\n",
    " {'batch_size': 8,\n",
    "  'epochs': 100,\n",
    "  'loss': 2.6716349124908447,\n",
    "  'accuracy': 0.5737777948379517},\n",
    " {'batch_size': 32,\n",
    "  'epochs': 10,\n",
    "  'loss': 0.8014382719993591,\n",
    "  'accuracy': 0.5855555534362793},\n",
    " {'batch_size': 32,\n",
    "  'epochs': 50,\n",
    "  'loss': 1.791918396949768,\n",
    "  'accuracy': 0.5933333039283752},\n",
    " {'batch_size': 32,\n",
    "  'epochs': 100,\n",
    "  'loss': 2.2307729721069336,\n",
    "  'accuracy': 0.5973333120346069},\n",
    " {'batch_size': 64,\n",
    "  'epochs': 10,\n",
    "  'loss': 0.725799024105072,\n",
    "  'accuracy': 0.6437777876853943},\n",
    " {'batch_size': 64,\n",
    "  'epochs': 50,\n",
    "  'loss': 1.4171788692474365,\n",
    "  'accuracy': 0.6006666421890259},\n",
    " {'batch_size': 64,\n",
    "  'epochs': 100,\n",
    "  'loss': 2.0028185844421387,\n",
    "  'accuracy': 0.6006666421890259}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.6594 - accuracy: 0.6004\n",
      "Epoch 00001: saving model to training/bert_training_individual_1\\cp.ckpt\n",
      "329/329 [==============================] - 88s 266ms/step - loss: 0.6594 - accuracy: 0.6003 - val_loss: 0.6572 - val_accuracy: 0.6369\n",
      "Epoch 2/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.6308 - accuracy: 0.6431\n",
      "Epoch 00002: saving model to training/bert_training_individual_1\\cp.ckpt\n",
      "329/329 [==============================] - 87s 265ms/step - loss: 0.6309 - accuracy: 0.6430 - val_loss: 0.6558 - val_accuracy: 0.6418\n",
      "Epoch 3/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.6128 - accuracy: 0.6644\n",
      "Epoch 00003: saving model to training/bert_training_individual_1\\cp.ckpt\n",
      "329/329 [==============================] - 87s 265ms/step - loss: 0.6129 - accuracy: 0.6643 - val_loss: 0.6577 - val_accuracy: 0.6456\n",
      "Epoch 4/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.5971 - accuracy: 0.6759\n",
      "Epoch 00004: saving model to training/bert_training_individual_1\\cp.ckpt\n",
      "329/329 [==============================] - 87s 265ms/step - loss: 0.5970 - accuracy: 0.6759 - val_loss: 0.6565 - val_accuracy: 0.6407\n",
      "Epoch 5/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.5816 - accuracy: 0.6888\n",
      "Epoch 00005: saving model to training/bert_training_individual_1\\cp.ckpt\n",
      "329/329 [==============================] - 87s 265ms/step - loss: 0.5816 - accuracy: 0.6889 - val_loss: 0.6641 - val_accuracy: 0.6451\n",
      "Epoch 6/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7079\n",
      "Epoch 00006: saving model to training/bert_training_individual_1\\cp.ckpt\n",
      "329/329 [==============================] - 87s 265ms/step - loss: 0.5618 - accuracy: 0.7078 - val_loss: 0.6865 - val_accuracy: 0.6400\n",
      "Epoch 7/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.5498 - accuracy: 0.7167\n",
      "Epoch 00007: saving model to training/bert_training_individual_1\\cp.ckpt\n",
      "329/329 [==============================] - 87s 265ms/step - loss: 0.5497 - accuracy: 0.7168 - val_loss: 0.6756 - val_accuracy: 0.6453\n",
      "Epoch 8/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.5317 - accuracy: 0.7300\n",
      "Epoch 00008: saving model to training/bert_training_individual_1\\cp.ckpt\n",
      "329/329 [==============================] - 87s 265ms/step - loss: 0.5316 - accuracy: 0.7301 - val_loss: 0.6962 - val_accuracy: 0.6389\n",
      "Epoch 9/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.5164 - accuracy: 0.7421\n",
      "Epoch 00009: saving model to training/bert_training_individual_1\\cp.ckpt\n",
      "329/329 [==============================] - 87s 265ms/step - loss: 0.5164 - accuracy: 0.7420 - val_loss: 0.7110 - val_accuracy: 0.6480\n",
      "Epoch 10/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.4997 - accuracy: 0.7554\n",
      "Epoch 00010: saving model to training/bert_training_individual_1\\cp.ckpt\n",
      "329/329 [==============================] - 87s 265ms/step - loss: 0.4997 - accuracy: 0.7554 - val_loss: 0.7152 - val_accuracy: 0.6384\n"
     ]
    }
   ],
   "source": [
    "with tf.device('gpu:0'):\n",
    "    # Fit\n",
    "    bert_model_individual.fit(\n",
    "        x=tweet_individual_train, \n",
    "        y=label_individual_train, \n",
    "        batch_size=best_result['batch_size'], \n",
    "        epochs=best_result['epochs'], \n",
    "        callbacks=[bert_checkpoint_callback_individual],\n",
    "        validation_data=(tweet_individual_val, label_individual_val),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 6s 42ms/step - loss: 0.7464 - accuracy: 0.5804\n"
     ]
    }
   ],
   "source": [
    "eval_result = bert_model_individual.evaluate(tweet_individual_test, label_individual_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_email(\n",
    "    f\"\"\"\n",
    "    Individual model fit finished.\n",
    "    > loss: {eval_result[0]}\n",
    "    > accuracy: {eval_result[1]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating for each user (rather than each tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate user predictions from individual tweet data\n",
    "def calculate_user_predictions_from_individual_tweets(model, tweets, labels):\n",
    "    # Get tweets for each user\n",
    "    tweets_per_user = 100\n",
    "    user_tweets = [\n",
    "        {\n",
    "            'input_word_ids': tweets['input_word_ids'][i:i+tweets_per_user],\n",
    "            'input_mask': tweets['input_mask'][i:i+tweets_per_user],\n",
    "            'input_type_ids': tweets['input_type_ids'][i:i+tweets_per_user],\n",
    "        }\n",
    "        for i in range(0, len(tweets['input_word_ids']), tweets_per_user)\n",
    "    ]\n",
    "    user_labels = np.asarray([\n",
    "        labels[i].numpy() for i in range(0, len(labels), tweets_per_user)\n",
    "     ])\n",
    "    \n",
    "    # Evaluate each user\n",
    "    all_predictions = []\n",
    "    for user_label, user_tweet in zip(user_labels, user_tweets):\n",
    "        all_predictions.append(\n",
    "            model.predict(user_tweet).flatten()\n",
    "        )\n",
    "    \n",
    "    return np.asarray(all_predictions), user_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_true_positive(label, prediction):\n",
    "    return label == 1 and prediction == 1\n",
    "    \n",
    "def is_false_positive(label, prediction):\n",
    "    return label == 0 and prediction == 1\n",
    "\n",
    "def is_false_negative(label, prediction):\n",
    "    return label == 1 and prediction == 0 \n",
    "    \n",
    "def is_true_negative(label, prediction):\n",
    "    return label == 0 and prediction == 0\n",
    "\n",
    "# Evaluate the model, returning accuracy, recall, f1, etc\n",
    "# predictions should be of type [(label, [predictions])]\n",
    "def evaluate_model(predictions, labels):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        # Take the mean of the users predictions and compare to threshold\n",
    "        if is_true_positive(label, prediction):\n",
    "            tp += 1\n",
    "        elif is_false_positive(label, prediction):\n",
    "            fp += 1\n",
    "        elif is_false_negative(label, prediction):\n",
    "            fn += 1\n",
    "        elif is_true_negative(label, prediction):\n",
    "            tn += 1\n",
    "        else:\n",
    "            print(\"Error:\", label, prediction)\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else float(\"NaN\")\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else float(\"NaN\")\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision > 0 and recall > 0 else float(\"NaN\")\n",
    "            \n",
    "    return {\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn,\n",
    "        'true_negatives': tn,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_model(trainable=True):\n",
    "    bert_encoder_individual_test = hub.KerasLayer(\n",
    "        small_bert_url, \n",
    "        trainable=trainable,\n",
    "    )\n",
    "    bert_model_individual_test = bclf.create_bert_model(bert_encoder_individual_test, bert_input_size_individual)\n",
    "    bert_model_individual_test.compile(Adam(lr=1e-5), 'binary_crossentropy', ['accuracy'])\n",
    "    return bert_encoder_individual_test, bert_model_individual_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   1/2625 [..............................] - ETA: 1:10 - loss: 0.6308 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0160s vs `on_train_batch_end` time: 0.0290s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0160s vs `on_train_batch_end` time: 0.0290s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2625/2625 [==============================] - ETA: 0s - loss: 0.6508 - accuracy: 0.6153\n",
      "Epoch 00001: saving model to training/bert_individual/batch8-epoch10-2\\cp.ckpt\n",
      "2625/2625 [==============================] - 107s 41ms/step - loss: 0.6508 - accuracy: 0.6153\n",
      "Epoch 2/10\n",
      "2625/2625 [==============================] - ETA: 0s - loss: 0.6046 - accuracy: 0.6673\n",
      "Epoch 00002: saving model to training/bert_individual/batch8-epoch10-2\\cp.ckpt\n",
      "2625/2625 [==============================] - 108s 41ms/step - loss: 0.6046 - accuracy: 0.6673\n",
      "Epoch 3/10\n",
      "2625/2625 [==============================] - ETA: 0s - loss: 0.5679 - accuracy: 0.7029\n",
      "Epoch 00003: saving model to training/bert_individual/batch8-epoch10-2\\cp.ckpt\n",
      "2625/2625 [==============================] - 107s 41ms/step - loss: 0.5679 - accuracy: 0.7029\n",
      "Epoch 4/10\n",
      "2625/2625 [==============================] - ETA: 0s - loss: 0.5316 - accuracy: 0.7358\n",
      "Epoch 00004: saving model to training/bert_individual/batch8-epoch10-2\\cp.ckpt\n",
      "2625/2625 [==============================] - 107s 41ms/step - loss: 0.5316 - accuracy: 0.7358\n",
      "Epoch 5/10\n",
      "2625/2625 [==============================] - ETA: 0s - loss: 0.5011 - accuracy: 0.7546\n",
      "Epoch 00005: saving model to training/bert_individual/batch8-epoch10-2\\cp.ckpt\n",
      "2625/2625 [==============================] - 107s 41ms/step - loss: 0.5011 - accuracy: 0.7546\n",
      "Epoch 6/10\n",
      "2625/2625 [==============================] - ETA: 0s - loss: 0.4671 - accuracy: 0.7750\n",
      "Epoch 00006: saving model to training/bert_individual/batch8-epoch10-2\\cp.ckpt\n",
      "2625/2625 [==============================] - 107s 41ms/step - loss: 0.4671 - accuracy: 0.7750\n",
      "Epoch 7/10\n",
      "2625/2625 [==============================] - ETA: 0s - loss: 0.4400 - accuracy: 0.7923\n",
      "Epoch 00007: saving model to training/bert_individual/batch8-epoch10-2\\cp.ckpt\n",
      "2625/2625 [==============================] - 107s 41ms/step - loss: 0.4400 - accuracy: 0.7923\n",
      "Epoch 8/10\n",
      "2625/2625 [==============================] - ETA: 0s - loss: 0.4087 - accuracy: 0.8102\n",
      "Epoch 00008: saving model to training/bert_individual/batch8-epoch10-2\\cp.ckpt\n",
      "2625/2625 [==============================] - 107s 41ms/step - loss: 0.4087 - accuracy: 0.8102\n",
      "Epoch 9/10\n",
      "2625/2625 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.8289\n",
      "Epoch 00009: saving model to training/bert_individual/batch8-epoch10-2\\cp.ckpt\n",
      "2625/2625 [==============================] - 107s 41ms/step - loss: 0.3807 - accuracy: 0.8289\n",
      "Epoch 10/10\n",
      "2625/2625 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.8443\n",
      "Epoch 00010: saving model to training/bert_individual/batch8-epoch10-2\\cp.ckpt\n",
      "2625/2625 [==============================] - 107s 41ms/step - loss: 0.3548 - accuracy: 0.8443\n",
      "Epoch 1/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.6614 - accuracy: 0.6000\n",
      "Epoch 00001: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.6613 - accuracy: 0.6000\n",
      "Epoch 2/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.6254 - accuracy: 0.6505\n",
      "Epoch 00002: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.6254 - accuracy: 0.6505\n",
      "Epoch 3/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.5992 - accuracy: 0.6759\n",
      "Epoch 00003: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.5991 - accuracy: 0.6759\n",
      "Epoch 4/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.5774 - accuracy: 0.6946\n",
      "Epoch 00004: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.5775 - accuracy: 0.6945\n",
      "Epoch 5/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.5550 - accuracy: 0.7134\n",
      "Epoch 00005: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.5549 - accuracy: 0.7135\n",
      "Epoch 6/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.5297 - accuracy: 0.7307\n",
      "Epoch 00006: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.5297 - accuracy: 0.7307\n",
      "Epoch 7/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.5085 - accuracy: 0.7505\n",
      "Epoch 00007: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.5085 - accuracy: 0.7505\n",
      "Epoch 8/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.4907 - accuracy: 0.7606\n",
      "Epoch 00008: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.4907 - accuracy: 0.7607\n",
      "Epoch 9/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.4708 - accuracy: 0.7731\n",
      "Epoch 00009: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.4707 - accuracy: 0.7731\n",
      "Epoch 10/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.4483 - accuracy: 0.7924\n",
      "Epoch 00010: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.4483 - accuracy: 0.7924\n",
      "Epoch 11/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.4309 - accuracy: 0.8037\n",
      "Epoch 00011: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.4310 - accuracy: 0.8037\n",
      "Epoch 12/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.4132 - accuracy: 0.8134\n",
      "Epoch 00012: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.4132 - accuracy: 0.8133\n",
      "Epoch 13/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.3969 - accuracy: 0.8208\n",
      "Epoch 00013: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.3971 - accuracy: 0.8207\n",
      "Epoch 14/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.3808 - accuracy: 0.8285\n",
      "Epoch 00014: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.3808 - accuracy: 0.8284\n",
      "Epoch 15/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.3649 - accuracy: 0.8409\n",
      "Epoch 00015: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.3648 - accuracy: 0.8410\n",
      "Epoch 16/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.3509 - accuracy: 0.8427\n",
      "Epoch 00016: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.3510 - accuracy: 0.8427\n",
      "Epoch 17/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.3288 - accuracy: 0.8554\n",
      "Epoch 00017: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.3287 - accuracy: 0.8554\n",
      "Epoch 18/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.3209 - accuracy: 0.8613\n",
      "Epoch 00018: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.3209 - accuracy: 0.8613\n",
      "Epoch 19/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8691\n",
      "Epoch 00019: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.3050 - accuracy: 0.8691\n",
      "Epoch 20/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.8760\n",
      "Epoch 00020: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.2917 - accuracy: 0.8759\n",
      "Epoch 21/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.2766 - accuracy: 0.8825\n",
      "Epoch 00021: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.2766 - accuracy: 0.8825\n",
      "Epoch 22/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.2652 - accuracy: 0.8904\n",
      "Epoch 00022: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.2651 - accuracy: 0.8904\n",
      "Epoch 23/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.2529 - accuracy: 0.8950\n",
      "Epoch 00023: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.2531 - accuracy: 0.8949\n",
      "Epoch 24/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.2400 - accuracy: 0.9009\n",
      "Epoch 00024: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.2400 - accuracy: 0.9009\n",
      "Epoch 25/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.2302 - accuracy: 0.9038\n",
      "Epoch 00025: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.2302 - accuracy: 0.9037\n",
      "Epoch 26/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.2240 - accuracy: 0.9079\n",
      "Epoch 00026: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.2239 - accuracy: 0.9079\n",
      "Epoch 27/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.2133 - accuracy: 0.9137\n",
      "Epoch 00027: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.2132 - accuracy: 0.9138\n",
      "Epoch 28/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1972 - accuracy: 0.9205\n",
      "Epoch 00028: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1972 - accuracy: 0.9205\n",
      "Epoch 29/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1896 - accuracy: 0.9246\n",
      "Epoch 00029: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1895 - accuracy: 0.9246\n",
      "Epoch 30/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1812 - accuracy: 0.9278\n",
      "Epoch 00030: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1812 - accuracy: 0.9277\n",
      "Epoch 31/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1708 - accuracy: 0.9303\n",
      "Epoch 00031: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1709 - accuracy: 0.9303\n",
      "Epoch 32/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1697 - accuracy: 0.9320\n",
      "Epoch 00032: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1698 - accuracy: 0.9319\n",
      "Epoch 33/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1574 - accuracy: 0.9384\n",
      "Epoch 00033: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1574 - accuracy: 0.9384\n",
      "Epoch 34/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1513 - accuracy: 0.9406\n",
      "Epoch 00034: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1513 - accuracy: 0.9406\n",
      "Epoch 35/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1479 - accuracy: 0.9419\n",
      "Epoch 00035: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1479 - accuracy: 0.9419\n",
      "Epoch 36/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.9440\n",
      "Epoch 00036: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1405 - accuracy: 0.9440\n",
      "Epoch 37/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1320 - accuracy: 0.9489\n",
      "Epoch 00037: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1320 - accuracy: 0.9489\n",
      "Epoch 38/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9516\n",
      "Epoch 00038: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1255 - accuracy: 0.9516\n",
      "Epoch 39/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9512\n",
      "Epoch 00039: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 133ms/step - loss: 0.1245 - accuracy: 0.9512\n",
      "Epoch 40/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1187 - accuracy: 0.9540\n",
      "Epoch 00040: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1187 - accuracy: 0.9540\n",
      "Epoch 41/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9570\n",
      "Epoch 00041: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 91s 138ms/step - loss: 0.1085 - accuracy: 0.9570\n",
      "Epoch 42/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1100 - accuracy: 0.9570\n",
      "Epoch 00042: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1100 - accuracy: 0.9570\n",
      "Epoch 43/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1025 - accuracy: 0.9588\n",
      "Epoch 00043: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1024 - accuracy: 0.9589\n",
      "Epoch 44/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.1009 - accuracy: 0.9606\n",
      "Epoch 00044: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.1009 - accuracy: 0.9606\n",
      "Epoch 45/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0961 - accuracy: 0.9629\n",
      "Epoch 00045: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0961 - accuracy: 0.9629\n",
      "Epoch 46/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0965 - accuracy: 0.9626\n",
      "Epoch 00046: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0965 - accuracy: 0.9626\n",
      "Epoch 47/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0904 - accuracy: 0.9653\n",
      "Epoch 00047: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0904 - accuracy: 0.9653\n",
      "Epoch 48/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0844 - accuracy: 0.9676\n",
      "Epoch 00048: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0844 - accuracy: 0.9676\n",
      "Epoch 49/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0836 - accuracy: 0.9682\n",
      "Epoch 00049: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0836 - accuracy: 0.9682\n",
      "Epoch 50/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0815 - accuracy: 0.9707\n",
      "Epoch 00050: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0816 - accuracy: 0.9706\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656/657 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9708\n",
      "Epoch 00051: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0753 - accuracy: 0.9708\n",
      "Epoch 52/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.9698\n",
      "Epoch 00052: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0790 - accuracy: 0.9698\n",
      "Epoch 53/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9726\n",
      "Epoch 00053: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0745 - accuracy: 0.9726\n",
      "Epoch 54/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 0.9737\n",
      "Epoch 00054: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0681 - accuracy: 0.9737\n",
      "Epoch 55/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0699 - accuracy: 0.9730\n",
      "Epoch 00055: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0699 - accuracy: 0.9730\n",
      "Epoch 56/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.9754\n",
      "Epoch 00056: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0684 - accuracy: 0.9754\n",
      "Epoch 57/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0651 - accuracy: 0.9764\n",
      "Epoch 00057: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0651 - accuracy: 0.9764\n",
      "Epoch 58/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9765\n",
      "Epoch 00058: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0659 - accuracy: 0.9764\n",
      "Epoch 59/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9752\n",
      "Epoch 00059: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0641 - accuracy: 0.9752\n",
      "Epoch 60/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9773\n",
      "Epoch 00060: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0608 - accuracy: 0.9773\n",
      "Epoch 61/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0574 - accuracy: 0.9780\n",
      "Epoch 00061: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0574 - accuracy: 0.9780\n",
      "Epoch 62/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0578 - accuracy: 0.9790\n",
      "Epoch 00062: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0580 - accuracy: 0.9790\n",
      "Epoch 63/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0542 - accuracy: 0.9801\n",
      "Epoch 00063: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0543 - accuracy: 0.9800\n",
      "Epoch 64/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9801\n",
      "Epoch 00064: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0560 - accuracy: 0.9801\n",
      "Epoch 65/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0532 - accuracy: 0.9796\n",
      "Epoch 00065: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0534 - accuracy: 0.9795\n",
      "Epoch 66/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0523 - accuracy: 0.9798\n",
      "Epoch 00066: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0523 - accuracy: 0.9798\n",
      "Epoch 67/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0518 - accuracy: 0.9804\n",
      "Epoch 00067: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0518 - accuracy: 0.9804\n",
      "Epoch 68/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0511 - accuracy: 0.9804\n",
      "Epoch 00068: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0511 - accuracy: 0.9804\n",
      "Epoch 69/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0470 - accuracy: 0.9824\n",
      "Epoch 00069: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0470 - accuracy: 0.9824\n",
      "Epoch 70/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0453 - accuracy: 0.9832\n",
      "Epoch 00070: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0453 - accuracy: 0.9831\n",
      "Epoch 71/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0449 - accuracy: 0.9832\n",
      "Epoch 00071: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0449 - accuracy: 0.9832\n",
      "Epoch 72/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0427 - accuracy: 0.9840\n",
      "Epoch 00072: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0426 - accuracy: 0.9840\n",
      "Epoch 73/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0450 - accuracy: 0.9840\n",
      "Epoch 00073: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0450 - accuracy: 0.9840\n",
      "Epoch 74/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0420 - accuracy: 0.9847\n",
      "Epoch 00074: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0420 - accuracy: 0.9847\n",
      "Epoch 75/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0425 - accuracy: 0.9846\n",
      "Epoch 00075: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0425 - accuracy: 0.9846\n",
      "Epoch 76/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0423 - accuracy: 0.9840\n",
      "Epoch 00076: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0423 - accuracy: 0.9840\n",
      "Epoch 77/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9856\n",
      "Epoch 00077: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0385 - accuracy: 0.9856\n",
      "Epoch 78/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0376 - accuracy: 0.9857\n",
      "Epoch 00078: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0376 - accuracy: 0.9857\n",
      "Epoch 79/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0410 - accuracy: 0.9848\n",
      "Epoch 00079: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0409 - accuracy: 0.9848\n",
      "Epoch 80/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0364 - accuracy: 0.9869\n",
      "Epoch 00080: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0364 - accuracy: 0.9869\n",
      "Epoch 81/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0344 - accuracy: 0.9862\n",
      "Epoch 00081: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0343 - accuracy: 0.9862\n",
      "Epoch 82/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0374 - accuracy: 0.9861\n",
      "Epoch 00082: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0374 - accuracy: 0.9861\n",
      "Epoch 83/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0368 - accuracy: 0.9868\n",
      "Epoch 00083: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0367 - accuracy: 0.9868\n",
      "Epoch 84/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0342 - accuracy: 0.9875\n",
      "Epoch 00084: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0342 - accuracy: 0.9875\n",
      "Epoch 85/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0324 - accuracy: 0.9885\n",
      "Epoch 00085: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0324 - accuracy: 0.9885\n",
      "Epoch 86/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0355 - accuracy: 0.9860\n",
      "Epoch 00086: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0355 - accuracy: 0.9860\n",
      "Epoch 87/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0347 - accuracy: 0.9867\n",
      "Epoch 00087: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0347 - accuracy: 0.9867\n",
      "Epoch 88/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0343 - accuracy: 0.9870\n",
      "Epoch 00088: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0343 - accuracy: 0.9870\n",
      "Epoch 89/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0323 - accuracy: 0.9878\n",
      "Epoch 00089: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 87s 132ms/step - loss: 0.0323 - accuracy: 0.9878\n",
      "Epoch 90/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0310 - accuracy: 0.9884\n",
      "Epoch 00090: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 133ms/step - loss: 0.0311 - accuracy: 0.9883\n",
      "Epoch 91/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0323 - accuracy: 0.9889\n",
      "Epoch 00091: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 89s 135ms/step - loss: 0.0323 - accuracy: 0.9889\n",
      "Epoch 92/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0319 - accuracy: 0.9880\n",
      "Epoch 00092: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 135ms/step - loss: 0.0319 - accuracy: 0.9880\n",
      "Epoch 93/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0310 - accuracy: 0.9880\n",
      "Epoch 00093: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.0310 - accuracy: 0.9880\n",
      "Epoch 94/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0292 - accuracy: 0.9889\n",
      "Epoch 00094: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.0292 - accuracy: 0.9889\n",
      "Epoch 95/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0329 - accuracy: 0.9877\n",
      "Epoch 00095: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.0329 - accuracy: 0.9877\n",
      "Epoch 96/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0291 - accuracy: 0.9888\n",
      "Epoch 00096: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.0291 - accuracy: 0.9888\n",
      "Epoch 97/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0285 - accuracy: 0.9898\n",
      "Epoch 00097: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 135ms/step - loss: 0.0285 - accuracy: 0.9898\n",
      "Epoch 98/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0322 - accuracy: 0.9880\n",
      "Epoch 00098: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.0321 - accuracy: 0.9880\n",
      "Epoch 99/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0308 - accuracy: 0.9885\n",
      "Epoch 00099: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.0308 - accuracy: 0.9885\n",
      "Epoch 100/100\n",
      "656/657 [============================>.] - ETA: 0s - loss: 0.0274 - accuracy: 0.9886\n",
      "Epoch 00100: saving model to training/bert_individual/batch32-epoch100-2\\cp.ckpt\n",
      "657/657 [==============================] - 88s 134ms/step - loss: 0.0274 - accuracy: 0.9886\n",
      "Epoch 1/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.6753 - accuracy: 0.5743\n",
      "Epoch 00001: saving model to training/bert_individual/batch64-epoch10-2\\cp.ckpt\n",
      "329/329 [==============================] - 82s 248ms/step - loss: 0.6753 - accuracy: 0.5743\n",
      "Epoch 2/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.6466 - accuracy: 0.6212\n",
      "Epoch 00002: saving model to training/bert_individual/batch64-epoch10-2\\cp.ckpt\n",
      "329/329 [==============================] - 82s 249ms/step - loss: 0.6467 - accuracy: 0.6210\n",
      "Epoch 3/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.6282 - accuracy: 0.6424\n",
      "Epoch 00003: saving model to training/bert_individual/batch64-epoch10-2\\cp.ckpt\n",
      "329/329 [==============================] - 82s 249ms/step - loss: 0.6283 - accuracy: 0.6422\n",
      "Epoch 4/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.6113 - accuracy: 0.6607\n",
      "Epoch 00004: saving model to training/bert_individual/batch64-epoch10-2\\cp.ckpt\n",
      "329/329 [==============================] - 82s 249ms/step - loss: 0.6114 - accuracy: 0.6607\n",
      "Epoch 5/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.5930 - accuracy: 0.6788\n",
      "Epoch 00005: saving model to training/bert_individual/batch64-epoch10-2\\cp.ckpt\n",
      "329/329 [==============================] - 82s 249ms/step - loss: 0.5929 - accuracy: 0.6789\n",
      "Epoch 6/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.5728 - accuracy: 0.6973\n",
      "Epoch 00006: saving model to training/bert_individual/batch64-epoch10-2\\cp.ckpt\n",
      "329/329 [==============================] - 82s 249ms/step - loss: 0.5729 - accuracy: 0.6972\n",
      "Epoch 7/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.5570 - accuracy: 0.7108\n",
      "Epoch 00007: saving model to training/bert_individual/batch64-epoch10-2\\cp.ckpt\n",
      "329/329 [==============================] - 82s 249ms/step - loss: 0.5569 - accuracy: 0.7107\n",
      "Epoch 8/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7217\n",
      "Epoch 00008: saving model to training/bert_individual/batch64-epoch10-2\\cp.ckpt\n",
      "329/329 [==============================] - 82s 249ms/step - loss: 0.5393 - accuracy: 0.7217\n",
      "Epoch 9/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.5250 - accuracy: 0.7341\n",
      "Epoch 00009: saving model to training/bert_individual/batch64-epoch10-2\\cp.ckpt\n",
      "329/329 [==============================] - 82s 249ms/step - loss: 0.5249 - accuracy: 0.7341\n",
      "Epoch 10/10\n",
      "328/329 [============================>.] - ETA: 0s - loss: 0.5073 - accuracy: 0.7480\n",
      "Epoch 00010: saving model to training/bert_individual/batch64-epoch10-2\\cp.ckpt\n",
      "329/329 [==============================] - 82s 249ms/step - loss: 0.5073 - accuracy: 0.7481\n"
     ]
    }
   ],
   "source": [
    "# Train and save best individual models\n",
    "pairs = [(8, 10), (32, 100), (64, 10)]\n",
    "\n",
    "for batch_size, epoch in pairs:\n",
    "    with tf.device('gpu:0'):\n",
    "        # Create checkpoint\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            filepath=f\"training/bert_individual/batch{batch_size}-epoch{epoch}-2/cp.ckpt\",\n",
    "            save_weights_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        bert_encoder_individual_test = hub.KerasLayer(\n",
    "            small_bert_url, \n",
    "            trainable=True,\n",
    "        )\n",
    "        bert_model_individual_test = bclf.create_bert_model(bert_encoder_individual_test, bert_input_size_individual)\n",
    "        bert_model_individual_test.compile(Adam(lr=1e-5), 'binary_crossentropy', ['accuracy'])\n",
    "\n",
    "        bert_model_individual_test.fit(\n",
    "            x=tweet_individual_train, \n",
    "            y=label_individual_train, \n",
    "            batch_size=batch_size, \n",
    "            epochs=epoch, \n",
    "            callbacks=[checkpoint],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_email(\"Finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying BERT predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(clf, X_train, y_train, X_val, y_val, batch_size, epochs):\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_val)\n",
    "    return {\n",
    "        'batch_size': batch_size,\n",
    "        'epochs': epochs,\n",
    "        'predictions_val': predictions,\n",
    "        'labels_val': y_val,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for batch_size:8, epochs: 10\n",
      "Training LR\n",
      "Training LR sorted\n",
      "Training SVM\n",
      "Training SVM sorted\n",
      "Loading weights for batch_size:32, epochs: 100\n",
      "Training LR\n",
      "Training LR sorted\n",
      "Training SVM\n",
      "Training SVM sorted\n",
      "Loading weights for batch_size:64, epochs: 10\n",
      "Training LR\n",
      "Training LR sorted\n",
      "Training SVM\n",
      "Training SVM sorted\n"
     ]
    }
   ],
   "source": [
    "pairs = [(8, 10), (32, 100), (64, 10)]\n",
    "predictions = {\n",
    "    'logistic_regression': [],\n",
    "    'logistic_regression_sorted': [],\n",
    "    'svm': [],\n",
    "    'svm_sorted': [],\n",
    "}\n",
    "\n",
    "for batch_size, epoch in pairs:\n",
    "    # Load the BERT model\n",
    "    print(f\"Loading weights for batch_size:{batch_size}, epochs: {epoch}\")\n",
    "    encoder, model = create_test_model(trainable=False)\n",
    "    model.load_weights(\n",
    "        f\"training/bert_individual/batch{batch_size}-epoch{epoch}-2/cp.ckpt\"\n",
    "    ).expect_partial()\n",
    "    \n",
    "    # Predict training and validation set data\n",
    "    X_train, y_train = calculate_user_predictions_from_individual_tweets(\n",
    "        model, \n",
    "        tweet_individual_train, \n",
    "        label_individual_train,\n",
    "    )\n",
    "    X_val, y_val = calculate_user_predictions_from_individual_tweets(\n",
    "        model, \n",
    "        tweet_individual_val, \n",
    "        label_individual_val,\n",
    "    )\n",
    "    \n",
    "    # Train Logistic Regression model\n",
    "    print(\"Training LR\")\n",
    "    predictions['logistic_regression'].append(\n",
    "        train_classifier(\n",
    "            LogisticRegression(), \n",
    "            X_train, \n",
    "            y_train, \n",
    "            X_val, \n",
    "            y_val, \n",
    "            batch_size, \n",
    "            epoch\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Train Logistic Regression when training data sorted\n",
    "    print(\"Training LR sorted\")\n",
    "    X_train_sorted = np.sort(X_train, axis=1)\n",
    "    X_val_sorted = np.sort(X_val, axis=1)\n",
    "    predictions['logistic_regression_sorted'].append(\n",
    "        train_classifier(\n",
    "            LogisticRegression(), \n",
    "            X_train_sorted, \n",
    "            y_train, \n",
    "            X_val_sorted, \n",
    "            y_val, \n",
    "            batch_size, \n",
    "            epoch\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Train SVM model\n",
    "    print(\"Training SVM\")\n",
    "    predictions['svm'].append(\n",
    "        train_classifier(\n",
    "            SVC(probability=True), \n",
    "            X_train, \n",
    "            y_train, \n",
    "            X_val, \n",
    "            y_val, \n",
    "            batch_size, \n",
    "            epoch\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Train SVM when training data sorted\n",
    "    print(\"Training SVM sorted\")\n",
    "    predictions['svm_sorted'].append(\n",
    "        train_classifier(\n",
    "            SVC(probability=True), \n",
    "            X_train_sorted, \n",
    "            y_train, \n",
    "            X_val_sorted, \n",
    "            y_val, \n",
    "            batch_size, \n",
    "            epoch\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic_regression': [{'batch_size': 8,\n",
       "   'epochs': 10,\n",
       "   'predictions_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "          1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1]),\n",
       "   'labels_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1])},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 100,\n",
       "   'predictions_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "          1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1]),\n",
       "   'labels_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1])},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 10,\n",
       "   'predictions_val': array([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1]),\n",
       "   'labels_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1])}],\n",
       " 'logistic_regression_sorted': [{'batch_size': 8,\n",
       "   'epochs': 10,\n",
       "   'predictions_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1]),\n",
       "   'labels_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1])},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 100,\n",
       "   'predictions_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1]),\n",
       "   'labels_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1])},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 10,\n",
       "   'predictions_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "          1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1]),\n",
       "   'labels_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1])}],\n",
       " 'svm': [{'batch_size': 8,\n",
       "   'epochs': 10,\n",
       "   'predictions_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1]),\n",
       "   'labels_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1])},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 100,\n",
       "   'predictions_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "          1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1]),\n",
       "   'labels_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1])},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 10,\n",
       "   'predictions_val': array([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "          1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1]),\n",
       "   'labels_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1])}],\n",
       " 'svm_sorted': [{'batch_size': 8,\n",
       "   'epochs': 10,\n",
       "   'predictions_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1]),\n",
       "   'labels_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1])},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 100,\n",
       "   'predictions_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1]),\n",
       "   'labels_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1])},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 10,\n",
       "   'predictions_val': array([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "          1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1]),\n",
       "   'labels_val': array([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "          1])}]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_email(f\"Finished training, predictions:\\n{predictions}\")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "for key, val in predictions.items():\n",
    "    for clf_info in val:\n",
    "        res = []\n",
    "        res.append(key)\n",
    "        res.append(clf_info['batch_size'])\n",
    "        res.append(clf_info['epochs'])\n",
    "        \n",
    "        eval_scores = evaluate_model(\n",
    "            clf_info['predictions_val'], \n",
    "            clf_info['labels_val']\n",
    "        )\n",
    "        res.append(eval_scores['accuracy'])\n",
    "        res.append(eval_scores['precision'])\n",
    "        res.append(eval_scores['recall'])\n",
    "        res.append(eval_scores['f1'])\n",
    "        results.append(res)\n",
    "\n",
    "df = pd.DataFrame(results, columns=['final classifier', 'batch_size', 'epochs', 'accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final classifier</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>svm_sorted</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.765957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svm</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.739130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logistic_regression_sorted</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.734694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.723404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>svm_sorted</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.723404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.711111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic_regression_sorted</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.711111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>svm</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.697674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logistic_regression_sorted</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.682927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>svm</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svm_sorted</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              final classifier  batch_size  epochs  accuracy  precision  \\\n",
       "1          logistic_regression          32     100  0.777778   0.750000   \n",
       "9                   svm_sorted           8      10  0.755556   0.782609   \n",
       "7                          svm          32     100  0.733333   0.772727   \n",
       "4   logistic_regression_sorted          32     100  0.711111   0.720000   \n",
       "2          logistic_regression          64      10  0.711111   0.739130   \n",
       "10                  svm_sorted          32     100  0.711111   0.739130   \n",
       "0          logistic_regression           8      10  0.711111   0.761905   \n",
       "3   logistic_regression_sorted           8      10  0.711111   0.761905   \n",
       "8                          svm          64      10  0.711111   0.789474   \n",
       "5   logistic_regression_sorted          64      10  0.711111   0.823529   \n",
       "6                          svm           8      10  0.688889   0.750000   \n",
       "11                  svm_sorted          64      10  0.688889   0.777778   \n",
       "\n",
       "      recall        f1  \n",
       "1   0.875000  0.807692  \n",
       "9   0.750000  0.765957  \n",
       "7   0.708333  0.739130  \n",
       "4   0.750000  0.734694  \n",
       "2   0.708333  0.723404  \n",
       "10  0.708333  0.723404  \n",
       "0   0.666667  0.711111  \n",
       "3   0.666667  0.711111  \n",
       "8   0.625000  0.697674  \n",
       "5   0.583333  0.682927  \n",
       "6   0.625000  0.681818  \n",
       "11  0.583333  0.666667  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['accuracy', 'f1'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_email(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the probability of a user being a fake news spreader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for batch_size: 32, epochs: 100\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LR\n",
      "Training SVM\n",
      "Training LR sorted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the BERT model\n",
    "print(f\"Loading weights for batch_size: 32, epochs: 100\")\n",
    "encoder, model = create_test_model()\n",
    "model.load_weights(\n",
    "    f\"training/bert_individual/batch32-epoch100/cp.ckpt\"\n",
    ").expect_partial()\n",
    "\n",
    "# Predict training and validation set data\n",
    "X_train, y_train = calculate_user_predictions_from_individual_tweets(\n",
    "    model, \n",
    "    tweet_individual_train, \n",
    "    label_individual_train,\n",
    ")\n",
    "X_val, y_val = calculate_user_predictions_from_individual_tweets(\n",
    "    model, \n",
    "    tweet_individual_val, \n",
    "    label_individual_val,\n",
    ")\n",
    "\n",
    "# Train Logistic Regression model\n",
    "print(\"Training LR\")\n",
    "log_reg_clf = LogisticRegression()\n",
    "log_reg_clf.fit(X_train, y_train)\n",
    "\n",
    "# Train SVM\n",
    "print(\"Training SVM\")\n",
    "svm_clf = SVC(probability=True)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Train Logistic Regression when training data sorted\n",
    "print(\"Training LR sorted\")\n",
    "X_train_sorted = np.sort(X_train, axis=1)\n",
    "X_val_sorted = np.sort(X_val, axis=1)\n",
    "log_reg_sorted_clf = LogisticRegression()\n",
    "log_reg_sorted_clf.fit(X_train_sorted, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR User 1 predict probability: [[0.52118894 0.47881106]]\n",
      "LR User 1 prediction: [0]\n",
      "LR sorted User 1 predict probability: [[0.74938058 0.25061942]]\n",
      "LR sorted User 1 prediction: [0]\n",
      "SVM User 1 predict probability: [[0.792287 0.207713]]\n",
      "SVM User 1 prediction: [0]\n",
      "User 1 label: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"LR User 1 predict probability:\", \n",
    "      log_reg_clf.predict_proba([X_val[0]]))\n",
    "print(\"LR User 1 prediction:\", log_reg_clf.predict([X_val[0]]))\n",
    "print(\"LR sorted User 1 predict probability:\", \n",
    "      log_reg_sorted_clf.predict_proba([X_val_sorted[0]]))\n",
    "print(\"LR sorted User 1 prediction:\", log_reg_sorted_clf.predict([X_val_sorted[0]]))\n",
    "print(\"SVM User 1 predict probability:\", \n",
    "\n",
    "      svm_clf.predict_proba([X_val[0]]))\n",
    "print(\"SVM User 1 prediction:\", svm_clf.predict([X_val[0]]))\n",
    "print(\"User 1 label:\", y_val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training BERT individual + Logistic Regression sorted\n",
    "BERT Model:\n",
    "* BERT L-12, Input 128\n",
    "* Individual tweets\n",
    "* batch_size 32, epochs 100\n",
    "\n",
    "Logistic Regression Model:\n",
    "* Predict training set using BERT and sort each datapoint - this will be the LR training data\n",
    "* Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def test_bert_model(encoder, input_size, dropout=0.1):\n",
    "    # Create BERT input layers\n",
    "    def input_layer(input_name):\n",
    "        return Input(shape=(input_size,), dtype=tf.int32, name=input_name)\n",
    "\n",
    "    inputs = {\n",
    "        'input_word_ids': input_layer(\"inputs/input_word_ids\"),\n",
    "        'input_mask': input_layer(\"inputs/input_mask\"),\n",
    "        'input_type_ids': input_layer(\"inputs/input_type_ids\"),\n",
    "    }\n",
    "\n",
    "    # BERT's pooled output\n",
    "    encoder_pooled_output = encoder(inputs)['pooled_output']\n",
    "    \n",
    "    # Dropout\n",
    "    dropout_out = tf.keras.layers.Dropout(dropout)(encoder_pooled_output)\n",
    "\n",
    "    # Dense layer output\n",
    "    dense_output = Dense(1, activation='sigmoid')(dropout_out)\n",
    "\n",
    "    # Create the Keras model and compile\n",
    "    return Model(inputs, dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from official.nlp import optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   2/1313 [..............................] - ETA: 10:32 - loss: 0.7859 - binary_accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0480s vs `on_train_batch_end` time: 0.9142s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0480s vs `on_train_batch_end` time: 0.9142s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1214/1313 [==========================>...] - ETA: 7s - loss: 0.7042 - binary_accuracy: 0.5207"
     ]
    }
   ],
   "source": [
    "setup = [(b, e, lr) \n",
    "         for b in [16, 32, 48, 64] \n",
    "         for e in [10] \n",
    "         for lr in [5e-5, 3e-5, 2e-5]]\n",
    "\n",
    "for batch_size, epochs, learning_rate in setup:\n",
    "    \n",
    "    # TensorBoard callback for logging loss\n",
    "    model_name = f\"batch_size{batch_size}-epochs{epochs}-lr{learning_rate}\"\n",
    "    model_path = \"training/bert_individual/loss-testing-4\"\n",
    "    log_dir = model_path + \"logs/\" + model_name\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    \n",
    "    # Checkpoint to save model\n",
    "    checkpoint_path_individual = model_path + model_name + \"/cp.ckpt\"\n",
    "    bert_individual_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_path_individual,\n",
    "        save_weights_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Encoder\n",
    "    bert_individual_size = 128\n",
    "    bert_individual_encoder = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1\", \n",
    "        trainable=True,\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    num_train_steps = (epochs*len(tweet_individual_train['input_word_ids']))\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "    bert_optimizer = optimization.create_optimizer(\n",
    "        init_lr=learning_rate,\n",
    "        num_train_steps=num_train_steps,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        optimizer_type='adamw'\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    bert_individual_model = test_bert_model(\n",
    "        bert_individual_encoder, \n",
    "        bert_individual_size,\n",
    "    )\n",
    "    bert_individual_model.compile(\n",
    "        optimizer=bert_optimizer, \n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "        metrics=tf.metrics.BinaryAccuracy(),\n",
    "    )\n",
    "    \n",
    "    # Train BERT\n",
    "    bert_individual_model.fit(\n",
    "        x=tweet_individual_train,\n",
    "        y=label_individual_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[bert_individual_checkpoint_callback, tensorboard_callback],\n",
    "        validation_data=(tweet_individual_val, label_individual_val),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_email(\"Loss testing finished for individual model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 6s 46ms/step - loss: 2.3060 - accuracy: 0.5780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3060004711151123, 0.578000009059906]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_individual_model.evaluate(tweet_individual_val, label_individual_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1398e39eac8>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the above BERT model\n",
    "# bert_individual_model = bclf.create_bert_model(\n",
    "#     bert_individual_encoder, \n",
    "#     bert_individual_size,\n",
    "# )\n",
    "# bert_individual_model.load_weights(\n",
    "#     f\"training/bert_individual/batch32-epoch100/cp.ckpt\"\n",
    "# ).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training/bert_individual/best-batch_size32-epochs-100-2/logistic_regressor.joblib']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Train Logistic Regression classifier\n",
    "X_train, y_train = calculate_user_predictions_from_individual_tweets(\n",
    "    bert_individual_model, \n",
    "    tweet_individual_train, \n",
    "    label_individual_train,\n",
    ")\n",
    "X_train_sorted = np.sort(X_train, axis=1)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_sorted, y_train)\n",
    "dump(clf, model_path + \"logistic_regressor.joblib\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation eval:\n",
      "{'true_positives': 19, 'false_positives': 8, 'false_negatives': 5, 'true_negatives': 13, 'accuracy': 0.7111111111111111, 'precision': 0.7037037037037037, 'recall': 0.7916666666666666, 'f1': 0.7450980392156864}\n",
      "Test eval:\n",
      "{'true_positives': 14, 'false_positives': 12, 'false_negatives': 4, 'true_negatives': 15, 'accuracy': 0.6444444444444445, 'precision': 0.5384615384615384, 'recall': 0.7777777777777778, 'f1': 0.6363636363636364}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "X_val, y_val = calculate_user_predictions_from_individual_tweets(\n",
    "    bert_individual_model, \n",
    "    tweet_individual_val, \n",
    "    label_individual_val,\n",
    ")\n",
    "X_val_sorted = np.sort(X_val, axis=1)\n",
    "\n",
    "X_test, y_test = calculate_user_predictions_from_individual_tweets(\n",
    "    bert_individual_model, \n",
    "    tweet_individual_test, \n",
    "    label_individual_test,\n",
    ")\n",
    "X_test_sorted = np.sort(X_test, axis=1)\n",
    "\n",
    "pred_val = clf.predict(X_val_sorted)\n",
    "pred_test = clf.predict(X_test_sorted)\n",
    "\n",
    "result = f\"Validation eval:\\n{evaluate_model(pred_val, y_val)}\\nTest eval:\\n{evaluate_model(pred_test, y_test)}\"\n",
    "send_email(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Tweet Feed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_bert_url = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1\"\n",
    "bert_encoder_feed = hub.KerasLayer(\n",
    "    medium_bert_url, \n",
    "    trainable=True,\n",
    ")\n",
    "\n",
    "bert_input_size_feed = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_tokenizer = clf.BertTweetFeedTokenizer(bert_encoder_feed, bert_input_size_feed)\n",
    "bert_model_feed = bclf.create_bert_model(bert_encoder_feed, bert_input_size_feed)\n",
    "bert_model_feed.compile(Adam(lr=1e-5), 'binary_crossentropy', ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_feed_train = feed_tokenizer.tokenize_input(tweet_train)\n",
    "label_feed_train = feed_tokenizer.tokenize_labels(label_train)\n",
    "tweet_feed_val = feed_tokenizer.tokenize_input(tweet_val)\n",
    "label_feed_val = feed_tokenizer.tokenize_labels(label_val)\n",
    "tweet_feed_test = feed_tokenizer.tokenize_input(tweet_test)\n",
    "label_feed_test = feed_tokenizer.tokenize_labels(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_loss_testing(model_path, bert_size, bert_encoder_url, X_train, y_train, X_val, y_val):\n",
    "    setup = [(b, e, lr) \n",
    "             for b in [8, 16, 24, 32, 40] \n",
    "             for e in [10] \n",
    "             for lr in [5e-5, 3e-5, 2e-5, 1e-5]]\n",
    "\n",
    "    for batch_size, epochs, learning_rate in setup:\n",
    "        # TensorBoard callback for logging loss\n",
    "        model_name = f\"batch_size{batch_size}-epochs{epochs}-lr{learning_rate}\"\n",
    "        log_dir = model_path + \"logs/\" + model_name\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "        # Checkpoint to save model\n",
    "        checkpoint_path = model_path + model_name + \"/cp.ckpt\"\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            save_weights_only=True,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        # BERT model\n",
    "        bert_encoder = hub.KerasLayer(\n",
    "            bert_encoder_url, \n",
    "            trainable=True,\n",
    "        )\n",
    "        bert_model = bclf.create_bert_model(\n",
    "            bert_encoder, \n",
    "            bert_size,\n",
    "        )\n",
    "        bert_model.compile(\n",
    "            Adam(learning_rate=learning_rate), \n",
    "            'binary_crossentropy', \n",
    "            ['accuracy'],\n",
    "        )\n",
    "\n",
    "        # Train BERT\n",
    "        bert_model.fit(\n",
    "            x=X_train,\n",
    "            y=y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=[checkpoint_callback, tensorboard_callback],\n",
    "            validation_data=(X_val, y_val),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_loss_testing(\n",
    "    \"training/bert_feed/loss-testing/\", \n",
    "    bert_input_size_feed,    \n",
    "    medium_bert_url, \n",
    "    tweet_feed_train, \n",
    "    label_feed_train, \n",
    "    tweet_feed_val, \n",
    "    label_feed_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
