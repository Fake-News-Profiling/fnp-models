{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "preprocessing_path = 'C:\\\\Users\\\\joshh\\\\Desktop\\\\Uni\\\\Soton Uni - Yr 3\\\\COMP3200\\\\fake-news-profiling\\\\classifier\\\\preprocessing'\n",
    "if preprocessing_path not in sys.path:\n",
    "    sys.path.insert(1, preprocessing_path)\n",
    "\n",
    "notif_path = 'C:\\\\Users\\\\joshh\\\\Desktop\\\\Uni\\\\Soton Uni - Yr 3\\\\COMP3200\\\\fake-news-profiling\\\\classifier\\\\notifications'\n",
    "if notif_path not in sys.path:\n",
    "    sys.path.insert(1, notif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import ipynb.fs.full.parse_datasets as datasets\n",
    "import ipynb.fs.full.preprocessing as pp\n",
    "import ipynb.fs.full.bert_fake_news_classifier as bclf\n",
    "from ipynb.fs.full.notif_email import send_email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data, label_data = datasets.parse_dataset(\"datasets\", \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extractor\n",
    "* Extract statistical features from tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import string\n",
    "import demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats extraction helper functions\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "digits = set(\"0123456789\")\n",
    "printable = set(string.printable)\n",
    "punctuation = set(string.punctuation)\n",
    "punctuation.remove('#')\n",
    "\n",
    "\n",
    "def clean_text(text, remove_punc=True, remove_non_print=True, remove_emojis=True, \n",
    "              remove_digits=True):\n",
    "    \"\"\" Clean text by removing certain characters (e.g. punctuation) \"\"\"\n",
    "    if remove_emojis:\n",
    "        text = demoji.replace(text, \"\")\n",
    "        \n",
    "    chars = []\n",
    "    for char in text:\n",
    "        if not ((remove_punc and char in punctuation) or\n",
    "            (remove_non_print and char not in printable) or\n",
    "            (remove_digits and char in digits)):\n",
    "            chars.append(char)\n",
    "        \n",
    "    return \"\".join(chars)\n",
    "\n",
    "\n",
    "def polarity_scores(text):\n",
    "    polarity_dict = analyzer.polarity_scores(text)\n",
    "    return np.asarray([\n",
    "        polarity_dict['pos'],\n",
    "        polarity_dict['neu'],\n",
    "        polarity_dict['neg'],\n",
    "    ])\n",
    "\n",
    "\n",
    "def tweets_to_words(user_tweets):\n",
    "    return np.asarray([clean_text(tweet.text).split() for tweet in user_tweets])\n",
    "\n",
    "\n",
    "def std_dev(datapoints, mean, num_datapoints=100):\n",
    "    diff = datapoints - mean\n",
    "    return np.sqrt(np.sum(diff ** 2)/100)\n",
    "\n",
    "\n",
    "def average_tweet_lengths(user_tweets):\n",
    "    return np.mean([len(tweet) for tweet in user_tweets])\n",
    "\n",
    "\n",
    "def std_dev_tweet_lengths(user_tweets):\n",
    "    tweet_lens = [len(tweet) for tweet in user_tweets]\n",
    "    return std_dev(\n",
    "        np.asarray(tweet_lens),\n",
    "        np.mean(tweet_lens),\n",
    "    )\n",
    "\n",
    "\n",
    "def cased_chars(user_tweets, cased):\n",
    "    return [\n",
    "        sum([c.isupper() if cased else c.islower() for c in tweet.text]) \n",
    "        for tweet in user_tweets\n",
    "    ]\n",
    "\n",
    "\n",
    "def emoji_chars(user_tweets):\n",
    "    return [len(demoji.findall_list(tweet.text)) for tweet in user_tweets]\n",
    "\n",
    "\n",
    "def punctuation_chars(user_tweets):\n",
    "    return [\n",
    "        len([c for c in tweet.text if c in punctuation]) \n",
    "        for tweet in user_tweets\n",
    "    ]\n",
    "\n",
    "\n",
    "def array(data_func):\n",
    "    return lambda args: np.asarray([data_func(args)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractor functions (to be used in TweetStatsExtractor)\n",
    "def average_chars(user_tweets):\n",
    "    \"\"\" Returns the average tweet lengths, in characters, for the user \"\"\"\n",
    "    return average_tweet_lengths([tweet.text for tweet in user_tweets])\n",
    "\n",
    "\n",
    "def std_dev_chars(user_tweets):\n",
    "    \"\"\" Returns the standard deviations of tweet lengths, in characters, for the user \"\"\"\n",
    "    return std_dev_tweet_lengths([tweet.text for tweet in user_tweets])\n",
    "\n",
    "\n",
    "def average_words(user_tweets):\n",
    "    \"\"\" Returns the average tweet lengths, in words, for the user \"\"\"\n",
    "    return average_tweet_lengths(tweets_to_words(user_tweets))\n",
    "\n",
    "\n",
    "def std_dev_words(user_tweets):\n",
    "    \"\"\" Returns the standard deviations of tweet lengths, in words, for the user \"\"\"\n",
    "    return std_dev_tweet_lengths(tweets_to_words(user_tweets))\n",
    "\n",
    "\n",
    "def average_sentiment(user_tweets):\n",
    "    \"\"\" Returns the average sentiment scores of the user \"\"\"\n",
    "    return np.mean([polarity_scores(tweet.text) for tweet in user_tweets], axis=0)\n",
    "\n",
    "\n",
    "def average_word_lengths(user_tweets):\n",
    "    \"\"\" Returns the average length of words used by this user \"\"\"\n",
    "    return np.mean([\n",
    "        len(word) \n",
    "        for tweet in user_tweets\n",
    "        for word in clean_text(tweet.text).split()\n",
    "    ])\n",
    "\n",
    "\n",
    "def average_tags(user_tweets, tags=['RT', '#USER#', '#HASHTAG#', '#URL#']):\n",
    "    \"\"\" Returns the average number of tags used by this user \"\"\"\n",
    "    return np.mean([\n",
    "        np.asarray([tweet.text.count(tag) for tag in tags])\n",
    "                   for tweet in user_tweets\n",
    "    ], axis=0)\n",
    "\n",
    "\n",
    "def average_cased_chars(user_tweets):\n",
    "    \"\"\" Returns the average number of cased (uppercase) characters per tweet, for the user \"\"\"\n",
    "    return np.mean(cased_chars(user_tweets, True))\n",
    "    \n",
    "\n",
    "def std_dev_cased_chars(user_tweets):\n",
    "    \"\"\" Returns the standard deviation of cased characters per tweet, for the user \"\"\"\n",
    "    return std_dev(cased_chars(user_tweets, True), average_cased_chars(user_tweets))\n",
    "    \n",
    "    \n",
    "def average_uncased_chars(user_tweets):\n",
    "    \"\"\" Returns the average number of uncased (lowercase) characters per tweet, for the user \"\"\"\n",
    "    return np.mean(cased_chars(user_tweets, False))\n",
    "\n",
    "\n",
    "def std_dev_uncased_chars(user_tweets):\n",
    "    \"\"\" Returns the standard deviation of cased characters per tweet, for the user \"\"\"\n",
    "    return std_dev(cased_chars(user_tweets, False), average_uncased_chars(user_tweets))\n",
    "\n",
    "    \n",
    "def average_emojis(user_tweets):\n",
    "    \"\"\" Returns the average number of emojis per tweet, for the user \"\"\"\n",
    "    return np.mean(emoji_chars(user_tweets))\n",
    "    \n",
    "    \n",
    "def std_dev_emojis(user_tweets):\n",
    "    \"\"\" Returns the standard deviation of emojis per tweet, for the user \"\"\"\n",
    "    return std_dev(emoji_chars(user_tweets), average_emojis(user_tweets))\n",
    "\n",
    "\n",
    "def average_punctuation_chars(user_tweets):\n",
    "    \"\"\" Returns the average number of punctuation characters per tweet, for the user \"\"\"\n",
    "    return np.mean(punctuation_chars(user_tweets))\n",
    "\n",
    "    \n",
    "def average_emojis(user_tweets):\n",
    "    \"\"\" Returns the standard deviation of punctuation characters emojis per tweet, for the user \"\"\"\n",
    "    return std_dev(punctuation_chars(user_tweets), average_punctuation_chars(user_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetStatsExtractor:\n",
    "    def __init__(self, funcs=[\n",
    "        array(average_chars),\n",
    "        array(std_dev_chars),\n",
    "        array(average_words),\n",
    "        array(std_dev_words),\n",
    "        array(average_word_lengths),\n",
    "        array(average_cased_chars),\n",
    "        array(std_dev_cased_chars),\n",
    "        array(average_uncased_chars),\n",
    "        array(std_dev_uncased_chars),\n",
    "        array(average_emojis),\n",
    "        array(std_dev_emojis),\n",
    "        array(average_punctuation_chars),\n",
    "        array(average_emojis),\n",
    "        average_tags,\n",
    "        average_sentiment,\n",
    "    ]):\n",
    "        self.funcs = funcs\n",
    "    \n",
    "    def transform(self, X, normalize_data=True):\n",
    "        result = np.asarray([\n",
    "            np.concatenate([np.asarray(f(tweet_feed)) for f in self.funcs])\n",
    "            for tweet_feed in X\n",
    "        ])\n",
    "        \n",
    "        return normalize(result) if normalize_data else result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier models\n",
    "* Classify users based on their (normalized) statistical features\n",
    "* Used a GridSearch to find model optimal parameters as well as TweetStatsExtractor optimal parameters\n",
    "* We can look at model weights to find the stats which are least and most useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats data\n",
    "stats_extractor = TweetStatsExtractor()\n",
    "tweet_stats_data = stats_extractor.transform(tweet_data)\n",
    "\n",
    "(tweet_train, label_train, \n",
    " tweet_val, label_val, \n",
    " tweet_test, label_test) = datasets.split_dataset(tweet_stats_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy (LogisticRegression): 0.5777777777777777\n",
      "Val accuracy (SVC): 0.5111111111111111\n",
      "Val accuracy (RandomForestClassifier): 0.6666666666666666\n",
      "Val accuracy (GradientBoostingClassifier): 0.6222222222222222\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "log_reg_clf = LogisticRegression(\"l2\")\n",
    "log_reg_clf.fit(tweet_train, label_train)\n",
    "print(\"Val accuracy (LogisticRegression):\", log_reg_clf.score(tweet_val, label_val))\n",
    "\n",
    "# Support Vector Classifier\n",
    "svc_clf = SVC()\n",
    "svc_clf.fit(tweet_train, label_train)\n",
    "print(\"Val accuracy (SVC):\", svc_clf.score(tweet_val, label_val))\n",
    "\n",
    "# Random Forest\n",
    "forest_clf = RandomForestClassifier()\n",
    "forest_clf.fit(tweet_train, label_train)\n",
    "print(\"Val accuracy (RandomForestClassifier):\", forest_clf.score(tweet_val, label_val))\n",
    "\n",
    "# Gradient Boosting\n",
    "grad_boost_clf = GradientBoostingClassifier()\n",
    "grad_boost_clf.fit(tweet_train, label_train)\n",
    "print(\"Val accuracy (GradientBoostingClassifier):\", grad_boost_clf.score(tweet_val, label_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
