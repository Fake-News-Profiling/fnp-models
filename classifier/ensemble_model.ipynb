{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "preprocessing_path = 'C:\\\\Users\\\\joshh\\\\Desktop\\\\Uni\\\\Soton Year 3\\\\COMP3200\\\\fake-news-profiling\\\\classifier\\\\preprocessing'\n",
    "if preprocessing_path not in sys.path:\n",
    "    sys.path.insert(1, preprocessing_path)\n",
    "\n",
    "notif_path = 'C:\\\\Users\\\\joshh\\\\Desktop\\\\Uni\\\\Soton Year 3\\\\COMP3200\\\\fake-news-profiling\\\\classifier\\\\notifications'\n",
    "if notif_path not in sys.path:\n",
    "    sys.path.insert(1, notif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from ipynb.fs.full.parse_datasets import parse_dataset, split_dataset\n",
    "import ipynb.fs.full.preprocessing as process\n",
    "import ipynb.fs.full.bert_fake_news_classifier as bclf\n",
    "from ipynb.fs.full.notif_email import send_email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved dataset split\n",
    "def load_data():\n",
    "    return np.load(\"datasets/en_split_data.npy\", allow_pickle=True)\n",
    "\n",
    "(tweet_train, label_train, \n",
    " tweet_val, label_val, \n",
    " tweet_test, label_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML/XML tags\n",
    "preprocessor = process.BertTweetFeedDataPreprocessor(transformers=[process.replace_xml_and_html])\n",
    "tweet_train_processed = preprocessor.transform(tweet_train)\n",
    "tweet_val_processed = preprocessor.transform(tweet_val)\n",
    "tweet_test_processed = preprocessor.transform(tweet_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import string\n",
    "import demoji\n",
    "import re\n",
    "from pyphen import Pyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats extraction helper functions\n",
    "digits = set(\"0123456789\")\n",
    "printable = set(string.printable)\n",
    "punctuation = set(string.punctuation)\n",
    "punctuation.remove('#')\n",
    "\n",
    "pyphen = Pyphen(lang='en')\n",
    "\n",
    "\n",
    "def clean_text(text, remove_punc=True, remove_non_print=True, remove_emojis=True, \n",
    "              remove_digits=True, remove_tags=False):\n",
    "    \"\"\" Clean text by removing certain characters (e.g. punctuation) \"\"\"\n",
    "    if remove_emojis:\n",
    "        text = demoji.replace(text, \"\")\n",
    "        \n",
    "    chars = []\n",
    "    for char in text:\n",
    "        if not ((remove_punc and char in punctuation) or\n",
    "            (remove_non_print and char not in printable) or\n",
    "            (remove_digits and char in digits)):\n",
    "            chars.append(char)\n",
    "\n",
    "    cleaned = \"\".join(chars)\n",
    "    if remove_tags:\n",
    "        return re.sub('#[A-Z]+#', \"\", cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def tweets_to_words(user_tweets, **kwargs):\n",
    "    return [clean_text(tweet, **kwargs).split() for tweet in user_tweets]\n",
    "\n",
    "\n",
    "def std_dev(datapoints, mean, num_datapoints=100):\n",
    "    diff = datapoints - mean\n",
    "    return np.sqrt(np.sum(diff ** 2, axis=0)/100)\n",
    "\n",
    "\n",
    "def average_tweet_lengths(user_tweets):\n",
    "    return np.mean([len(tweet) for tweet in user_tweets])\n",
    "\n",
    "\n",
    "def std_dev_tweet_lengths(user_tweets):\n",
    "    tweet_lens = [len(tweet) for tweet in user_tweets]\n",
    "    return std_dev(np.asarray(tweet_lens), np.mean(tweet_lens))\n",
    "\n",
    "\n",
    "def cased_chars(user_tweets, cased):\n",
    "    return [\n",
    "        sum([c.isupper() if cased else c.islower() for c in tweet]) \n",
    "        for tweet in user_tweets\n",
    "    ]\n",
    "\n",
    "\n",
    "def emoji_chars(user_tweets):\n",
    "    \"\"\" Returns an array of lists of emojis used in each of the users tweets\"\"\"\n",
    "    return [demoji.findall_list(tweet) for tweet in user_tweets]\n",
    "\n",
    "\n",
    "def punctuation_chars(user_tweets):\n",
    "    return [\n",
    "        len([c for c in tweet if c in punctuation]) \n",
    "        for tweet in user_tweets\n",
    "    ]\n",
    "\n",
    "\n",
    "def syllables(word):\n",
    "    \"\"\" Counts the number of syllables in a word \"\"\"\n",
    "    return pyphen.inserted(word).count('-') + 1\n",
    "\n",
    "\n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "class TweetStatsExtractor:\n",
    "    def __init__(self, extractors):\n",
    "        if len(extractors) == 0:\n",
    "            raise Exception(\"Must pass at least one extracting function\")\n",
    "\n",
    "        self.extractors = extractors\n",
    "    \n",
    "    def transform(self, X, normalize_data=False):\n",
    "        result = []\n",
    "        for user_tweets in X:\n",
    "            if len(self.extractors) > 1:\n",
    "                result.append(np.concatenate([self._apply(f, user_tweets) for f in self.extractors]))\n",
    "            else:\n",
    "                result.append(self._apply(self.extractors[0], user_tweets))\n",
    "        \n",
    "        return np.asarray(normalize(result) if normalize_data else result)\n",
    "    \n",
    "    def _apply(self, extractor, data):\n",
    "        result = extractor(data)\n",
    "        if isinstance(result, Iterable):\n",
    "            return result\n",
    "        else:\n",
    "            return np.asarray([result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_features_by_importance(X, y, feature_names):\n",
    "    select = SelectKBest(k='all')\n",
    "    select.fit(X, y)\n",
    "    ordered_features = sorted(zip(feature_names, select.scores_), key=lambda v: v[1], reverse=True)\n",
    "    for feature, score in ordered_features:\n",
    "        print(f\"{feature}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_estimators = [LogisticRegression, SVC, RandomForestClassifier, GradientBoostingClassifier, KNeighborsClassifier]\n",
    "grid_search_param_grids = [\n",
    "    {\"Estimator__penalty\": [\"l1\", \"l2\"], \n",
    "     \"Estimator__C\": [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 768, 1024, 1280], \n",
    "     \"Estimator__solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]},\n",
    "    {\"Estimator__C\": [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 768, 1024, 1280], \n",
    "     \"Estimator__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"], \n",
    "     \"Estimator__probability\": [True]},\n",
    "    {\"Estimator__n_estimators\": [25, 50, 100, 200, 400, 800], \n",
    "     \"Estimator__criterion\": [\"gini\", \"entropy\"], \n",
    "     \"Estimator__min_samples_leaf\": [1, 2, 4, 6, 8, 10, 12, 14, 16]},\n",
    "    {\"Estimator__loss\": [\"deviance\", \"exponential\"], \n",
    "     \"Estimator__learning_rate\": [0.01, 0.05, 0.1, 0.2], \n",
    "     \"Estimator__n_estimators\": [25, 50, 100, 200, 400, 800], \n",
    "     \"Estimator__min_samples_leaf\": [1, 2, 4, 6, 8, 10, 12, 14, 16]},\n",
    "    {\"Estimator__n_neighbors\": [2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "     \"Estimator__weights\": [\"uniform\", \"distance\", ]},\n",
    "]\n",
    "\n",
    "def grid_search(X_train, y_train, X_val, y_val, estimators=grid_search_estimators, param_grids=grid_search_param_grids):\n",
    "    \"\"\" \n",
    "    Performs a GridSearchCV on the training data, and then evaluates using the validation data.\n",
    "    Uses a pipeline to find the best K features to use from the training data.\n",
    "    Returns a list of each estimator with their best parameters, as well as a dataframe containing \n",
    "    evaluation data.\n",
    "    \"\"\"\n",
    "    best_df = pd.DataFrame(columns=[\"Estimator\", \"K best features\", \"Mean CV Loss\", \"Mean CV F1\", \"Mean CV Accuracy\", \"Val Loss\", \"Val Precision\", \"Val Recall\", \"Val F1\", \"Val Accuracy\"])\n",
    "    best_params = []\n",
    "    ks = list(range(1, len(X_train[0])+1))\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    for i, (estimator, param_grid) in tqdm(enumerate(zip(estimators, param_grids)), desc=\"Estimators\", total=len(estimators)):\n",
    "        # Perform a GridSearchCV\n",
    "        param_grid['SelectKBest__k'] = ks\n",
    "        search = GridSearchCV(\n",
    "            Pipeline([('SelectKBest', SelectKBest()), ('Estimator', estimator())]), \n",
    "            param_grid, \n",
    "            n_jobs=-1, \n",
    "            scoring={\n",
    "                \"accuracy\": make_scorer(accuracy_score), \n",
    "                \"f1\": make_scorer(f1_score, pos_label=\"1\"),\n",
    "                \"loss\": make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
    "            }, \n",
    "            refit=\"loss\",\n",
    "        )\n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        # Collect results\n",
    "        best_index = search.cv_results_['params'].index(search.best_params_)\n",
    "        y_train_pred = search.predict(X_train)\n",
    "        y_val_pred = search.predict(X_val)\n",
    "        best_df.loc[i] = [estimator.__name__, \n",
    "                          search.best_params_['SelectKBest__k'], \n",
    "                          abs(search.cv_results_['mean_test_loss'][best_index]), \n",
    "                          search.cv_results_['mean_test_f1'][best_index],\n",
    "                          search.cv_results_['mean_test_accuracy'][best_index],\n",
    "                          log_loss(y_val, search.predict_proba(X_val)), \n",
    "                          precision_score(y_val, y_val_pred, pos_label=\"1\"),\n",
    "                          recall_score(y_val, y_val_pred, pos_label=\"1\"),\n",
    "                          f1_score(y_val, y_val_pred, pos_label=\"1\"),\n",
    "                          accuracy_score(y_val, y_val_pred)]\n",
    "        best_params.append((estimator.__name__, search.best_params_))\n",
    "    \n",
    "    return best_params, best_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability model\n",
    "* Extract statistical readability from user tweets:\n",
    "    * Number of tags (hashtags, mentions, URLs) (https://www.aclweb.org/anthology/U19-1003.pdf, http://ceur-ws.org/Vol-2696/paper_189.pdf)\n",
    "    * Number of emojis (https://www.aclweb.org/anthology/U19-1003.pdf, http://ceur-ws.org/Vol-2696/paper_189.pdf)\n",
    "    * Ratio of words to sentences and syllables to words:\n",
    "        * Flesch-Kincaid grade level, on corrected (and non-corrected) tweets (stripped of unicode and tags, spelling corrected) (https://www.aclweb.org/anthology/U19-1003.pdf)\n",
    "        * Flesch Reading Ease, modified for short tweet lengths (https://arxiv.org/ftp/arxiv/papers/1401/1401.6058.pdf, https://www.aclweb.org/anthology/U19-1003.pdf)\n",
    "        * Both of these measurements use the ratio of words to sentences and syllables to words. Since we are using these in a model which will apply weights to these ratios, we don't need to use these scoring functions. Instead we will just make features using the ratios themselves.\n",
    "        * Note that https://arxiv.org/ftp/arxiv/papers/1401/1401.6058.pdf found that tweets are poorly structured and so assumed that each tweet was a single sentence. This reduces the ratio to just the total number of words per tweet.\n",
    "    * Tweet lengths (in words and characters) (https://www.aclweb.org/anthology/U19-1003.pdf, http://ceur-ws.org/Vol-2696/paper_189.pdf)\n",
    "    * Type-token ratio (num_unique_words/total_num_words) (http://ceur-ws.org/Vol-2696/paper_189.pdf)\n",
    "    * Retweet ratio (num_retweets/total_num_tweets) (http://ceur-ws.org/Vol-2380/paper_263.pdf, http://ceur-ws.org/Vol-2380/paper_189.pdf)\n",
    "    * Number of truncated tweets (end with a ...) (http://ceur-ws.org/Vol-2696/paper_189.pdf, http://ceur-ws.org/Vol-2380/paper_189.pdf)\n",
    "    * Use of punctuation marks (!,?, etc) (http://ceur-ws.org/Vol-2380/paper_263.pdf)\n",
    "    * Use of numerical values\n",
    "    * Use of personal pronouns (https://sml.stanford.edu/ml/2008/01/hancock-dp-on-lying.pdf)\n",
    "    * Automated Readability Index (ratio of chars to words, and words to sentences)\n",
    "\n",
    "\n",
    "* \"An Ensemble Model Using N-grams and Statistical Features to Identify Fake News Spreaders on Twitter\" paper (http://ceur-ws.org/Vol-2696/paper_189.pdf) built a statistical model to support their N-gram model for this task (and won). \"FacTweet: Profiling Fake News Twitter Accounts\" used statistical information, such as emotions, style and sentiment to profile fake news spreading users.\n",
    "* In \"A stylometric Inquiry into Hyperpartisan and Fake News\" paper, they used 10 readability scores to help classify hyperpartisan news. \"Automatic Detection of Fake News\" paper also used readability features, such as the number of characters, complex words, long words, number of syllables, word types, and number of paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import reduce, partial\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractor functions (to be used in TweetStatsExtractor)\n",
    "def tag_counts(user_tweets, tags=['RT', '#USER#', '#HASHTAG#', '#URL#']):\n",
    "    \"\"\" Returns the average number of tag used, for each tag in tags \"\"\"\n",
    "    return np.mean([[tweet.count(tag) for tag in tags] for tweet in user_tweets], axis=0)\n",
    "\n",
    "\n",
    "def retweet_ratio(user_tweets):\n",
    "    \"\"\" Returns the ratio of retweets to regular tweets \"\"\"\n",
    "    retweets = 0\n",
    "    for tweet in user_tweets:\n",
    "        if tweet.startswith(\"RT\"):\n",
    "            retweets += 1\n",
    "    \n",
    "    return retweets / len(user_tweets)\n",
    "\n",
    "def emojis_count(user_tweets):\n",
    "    \"\"\" Returns the following emoji counts for this user: total number of emojis used, average number of emojis used \n",
    "    per tweet, type-token ratio of emojis (uniqueness of emojis used) \"\"\"\n",
    "    tweet_emojis = emoji_chars(user_tweets)\n",
    "    flattened_tweet_emojis = flatten(tweet_emojis)\n",
    "    \n",
    "    total_num_emojis = len(flattened_tweet_emojis)\n",
    "    mean_num_emojis = np.mean(list(map(len, tweet_emojis)))\n",
    "    emoji_type_token_ratio = (len(Counter(flattened_tweet_emojis)) / total_num_emojis) if total_num_emojis > 0 else 0\n",
    "    return np.asarray([total_num_emojis, mean_num_emojis, emoji_type_token_ratio])\n",
    "\n",
    "\n",
    "def syllables_to_words_ratios(user_tweets):\n",
    "    \"\"\" Returns the overall, average, min, and max ratios of the number of syllables to the number of words \"\"\"\n",
    "    tweet_words = tweets_to_words(user_tweets, remove_tags=True)\n",
    "    tweet_syllables = [sum(map(syllables, words)) for words in tweet_words]\n",
    "    per_tweet_ratios = [tweet_syllables[i] / max(1, len(tweet_words[i])) for i in range(len(tweet_words))]\n",
    "    \n",
    "    overall_ratio = sum(tweet_syllables) / max(1, sum(map(len, tweet_words)))\n",
    "    mean_ratio = np.mean(per_tweet_ratios)\n",
    "    min_ratio = min(per_tweet_ratios)\n",
    "    max_ratio = max(per_tweet_ratios)\n",
    "    return np.asarray([overall_ratio, mean_ratio, min_ratio, max_ratio])\n",
    "\n",
    "\n",
    "def average_tweet_lengths(user_tweets):\n",
    "    \"\"\" Returns the average tweet lengths in words and characters \"\"\"\n",
    "    mean_words = np.mean(list(map(len, tweets_to_words(user_tweets, remove_tags=True))))\n",
    "    mean_chars = np.mean(list(map(len, map(partial(clean_text, remove_tags=True), user_tweets))))\n",
    "    return np.asarray([mean_words, mean_chars])\n",
    "\n",
    "\n",
    "def word_type_to_token_ratio(user_tweets):\n",
    "    \"\"\" Returns the ratio of unique words to the total number of words in all of a users tweets \"\"\"\n",
    "    words = flatten(tweets_to_words(user_tweets, remove_tags=True))\n",
    "    return len(Counter(list(words))) / len(words)\n",
    "\n",
    "\n",
    "def truncated_tweets(user_tweets):\n",
    "    \"\"\" Returns the number of truncated tweets \"\"\"\n",
    "    count = 0\n",
    "    for tweet in user_tweets:\n",
    "        if re.match(\".*\\.\\.\\.(?: #URL#)?$\", tweet) is not None:\n",
    "            count += 1\n",
    "        \n",
    "    return count\n",
    "\n",
    "\n",
    "def punctuation_counts(user_tweets, punctuation_marks = \"!?,:\"):\n",
    "    \"\"\" Returns the average number of each punctuation character in the users tweets, for each punctuation character \n",
    "    in punctuation_marks. Also returns the punctuation type-to-token ratio of all of the users tweets \"\"\"\n",
    "    all_punc = [c for tweet in user_tweets \n",
    "                for c in clean_text(tweet, remove_punc=False, remove_tags=True) if c in punctuation]\n",
    "    punc_ttr = len(Counter(all_punc)) / max(1, len(all_punc))\n",
    "    punc_counts = [[tweet.count(punctuation) for punctuation in punctuation_marks] for tweet in user_tweets]\n",
    "    mean_punc_counts = np.mean(punc_counts, axis=0)\n",
    "    return np.concatenate([mean_punc_counts, [punc_ttr]])\n",
    "\n",
    "\n",
    "def number_counts(user_tweets):\n",
    "    \"\"\" Returns the following counts: average number of numerical values per tweet (e.g. \"7,000\"), average number of \n",
    "    monetary values per tweet (e.g. \"$90,000\", \"£90 Million\") \"\"\"\n",
    "    number_matcher = \"\\d+(?:,\\d+)*(?:\\.\\d+)?\"\n",
    "    mean_numbers = np.mean([\n",
    "        len(re.findall(f\"(?:^| )(?<![£$€]){number_matcher}\", tweet)) for tweet in user_tweets\n",
    "    ])\n",
    "    mean_money = np.mean([\n",
    "        len(re.findall(f\"[£$€]{number_matcher}\", tweet)) for tweet in user_tweets\n",
    "    ])\n",
    "    return np.asarray([mean_numbers, mean_money])\n",
    "\n",
    "\n",
    "def average_personal_pronouns(user_tweets):\n",
    "    \"\"\" Returns the average number of personal pronouns per tweets \"\"\"\n",
    "    personal_pronouns_count = []\n",
    "    for tweet_words in tweets_to_words(user_tweets, remove_tags=True):\n",
    "        count = 0\n",
    "        for tag in nltk.pos_tag(tweet_words):\n",
    "            if tag[1] == 'PRP':\n",
    "                count += 1\n",
    "        \n",
    "        personal_pronouns_count.append(count)\n",
    "\n",
    "    return np.mean(personal_pronouns_count)\n",
    "\n",
    "\n",
    "def char_to_words_ratio(user_tweets):\n",
    "    \"\"\" Returns the ratio of characters to words in the users tweets \"\"\"\n",
    "    chars = 0\n",
    "    words = 0\n",
    "    for tweet in user_tweets:\n",
    "        cleaned_tweet = clean_text(tweet, remove_digits=False, remove_tags=True)\n",
    "        chars += len(cleaned_tweet)\n",
    "        words += len(cleaned_tweet.split())\n",
    "    \n",
    "    chars -= words # don't want to count spaces in chars\n",
    "    return chars / max(1, words)\n",
    "\n",
    "\n",
    "def quote_counts(user_tweets):\n",
    "    \"\"\" Returns the total and average number of quotes used by the user \"\"\"\n",
    "    num_quotes = [len(re.findall(\"(?:^| )(?:“.*?”|‘.*?’|\\\".*?\\\"|\\'.*?\\')\", tweet)) for tweet in user_tweets]\n",
    "    return np.asarray([sum(num_quotes), np.mean(num_quotes)])\n",
    "\n",
    "\n",
    "def capitalisation_counts(user_tweets):\n",
    "    \"\"\" Returns the following counts: average number of words with a capitalised first letter, \n",
    "    average number of fully capitalised words \"\"\"\n",
    "    first_capitalised = []\n",
    "    fully_capitalised = []\n",
    "    for tweet in user_tweets:\n",
    "        cleaned_tweet = clean_text(tweet, remove_tags=True)\n",
    "        first_capitalised.append(len(re.findall(\"[A-Z][a-z]+\", cleaned_tweet)))\n",
    "        fully_capitalised.append(len(re.findall(\"[A-Z]{2,}[^\\w]\", cleaned_tweet)))\n",
    "    \n",
    "    return np.asarray([\n",
    "        np.mean(first_capitalised),\n",
    "        np.mean(fully_capitalised),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats data\n",
    "read_stats_extractor = TweetStatsExtractor(extractors=[\n",
    "    tag_counts,\n",
    "    retweet_ratio,\n",
    "    emojis_count,\n",
    "    syllables_to_words_ratios,\n",
    "    average_tweet_lengths,\n",
    "    word_type_to_token_ratio,\n",
    "    truncated_tweets,\n",
    "    punctuation_counts,\n",
    "    number_counts,\n",
    "    average_personal_pronouns,\n",
    "    char_to_words_ratio,\n",
    "    quote_counts,\n",
    "    capitalisation_counts,\n",
    "])\n",
    "tweet_read_stats_train = read_stats_extractor.transform(tweet_train_processed)\n",
    "tweet_read_stats_val = read_stats_extractor.transform(tweet_val_processed)\n",
    "tweet_read_stats_test = read_stats_extractor.transform(tweet_test_processed)\n",
    "\n",
    "read_stats_feature_names = [\n",
    "    # tag_counts\n",
    "    \"Average number of 'RT' tags per tweet\",\n",
    "    \"Average number of '#USER#' tags per tweet\",\n",
    "    \"Average number of '#HASHTAG#' tags per tweet\",\n",
    "    \"Average number of '#URL#' tags per tweet\",\n",
    "    # retweet_ratio\n",
    "    \"Ratio of retweets to tweets\",\n",
    "    # emojis_count\n",
    "    \"Total number of emojis\",\n",
    "    \"Average number of emojis per tweet\",\n",
    "    \"Total emoji type-token ratio\",\n",
    "    # syllables_to_words_ratios\n",
    "    \"Total syllables-words ratio\",\n",
    "    \"Mean syllables-words ratio\",\n",
    "    \"Min syllables-words ratio\",\n",
    "    \"Max syllables-words ratio\",\n",
    "    # average_tweet_lengths\n",
    "    \"Average tweet lengths in words\",\n",
    "    \"Average tweet lengths in characters\",\n",
    "    # word_type_to_token_ratio\n",
    "    \"Total word type-token ratio\",\n",
    "    # truncated_tweets\n",
    "    \"Number of truncated tweets\",\n",
    "    # punctuation_counts\n",
    "    \"Average number of !\",\n",
    "    \"Average number of ?\",\n",
    "    \"Average number of ,\",\n",
    "    \"Average number of :\",\n",
    "    \"Total punctuation type-token ratio\",\n",
    "    # number_counts\n",
    "    \"Average number of numerical values\",\n",
    "    \"Average number of monetary values\",\n",
    "    # average_personal_pronouns\n",
    "    \"Average number of personal pronouns\",\n",
    "    # char_to_words_ratio\n",
    "    \"Ratio of characters to words\",\n",
    "    # quote_counts\n",
    "    \"Total number of quotes\",\n",
    "    \"Average number of quotes\",\n",
    "    # capitalisation_counts\n",
    "    \"Average words with first letter capitalised\",\n",
    "    \"Average fully capitalised words\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word type-token ratio: 18.361945312482035\n",
      "Average number of '#USER#' tags per tweet: 12.305988286594392\n",
      "Average words with first letter capitalised: 11.502536290933788\n",
      "Total syllables-words ratio: 11.153815093586113\n",
      "Mean syllables-words ratio: 9.919091231443657\n",
      "Ratio of characters to words: 9.814376512675755\n",
      "Average number of 'RT' tags per tweet: 8.980146530701075\n",
      "Ratio of retweets to tweets: 8.254128808847812\n",
      "Total emoji type-token ratio: 7.602301878568357\n",
      "Average number of !: 7.072581610833925\n",
      "Average number of :: 6.567071993577053\n",
      "Average number of emojis per tweet: 5.975803015603129\n",
      "Total number of emojis: 5.9758030156031285\n",
      "Average number of ,: 3.8500113605898063\n",
      "Average number of personal pronouns: 3.355087546524117\n",
      "Total punctuation type-token ratio: 3.1205480049388554\n",
      "Average fully capitalised words: 1.8677821603954763\n",
      "Average number of ?: 1.7240135937909138\n",
      "Average number of '#HASHTAG#' tags per tweet: 1.3953444608925827\n",
      "Average number of '#URL#' tags per tweet: 1.3347565972110607\n",
      "Min syllables-words ratio: 1.2975563637475809\n",
      "Average tweet lengths in characters: 0.8099656256917468\n",
      "Average number of numerical values: 0.4868684985574095\n",
      "Number of truncated tweets: 0.34525097134474875\n",
      "Max syllables-words ratio: 0.25961461611561576\n",
      "Average number of monetary values: 0.015826572682577995\n",
      "Average tweet lengths in words: 0.009451205886963498\n",
      "Total number of quotes: 0.005626942574898601\n",
      "Average number of quotes: 0.005626942574849802\n"
     ]
    }
   ],
   "source": [
    "print_features_by_importance(tweet_read_stats_train, label_train, read_stats_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimators: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [39:27<00:00, 473.51s/it]\n"
     ]
    }
   ],
   "source": [
    "read_stats_search_best_params, read_stats_search_df = grid_search(\n",
    "    tweet_read_stats_train, label_train, tweet_read_stats_val, label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LogisticRegression',\n",
       "  {'Estimator__C': 1,\n",
       "   'Estimator__penalty': 'l1',\n",
       "   'Estimator__solver': 'liblinear',\n",
       "   'SelectKBest__k': 1}),\n",
       " ('SVC',\n",
       "  {'Estimator__C': 1,\n",
       "   'Estimator__kernel': 'rbf',\n",
       "   'Estimator__probability': True,\n",
       "   'SelectKBest__k': 18}),\n",
       " ('RandomForestClassifier',\n",
       "  {'Estimator__criterion': 'gini',\n",
       "   'Estimator__min_samples_leaf': 2,\n",
       "   'Estimator__n_estimators': 200,\n",
       "   'SelectKBest__k': 23}),\n",
       " ('GradientBoostingClassifier',\n",
       "  {'Estimator__learning_rate': 0.05,\n",
       "   'Estimator__loss': 'deviance',\n",
       "   'Estimator__min_samples_leaf': 16,\n",
       "   'Estimator__n_estimators': 50,\n",
       "   'SelectKBest__k': 15}),\n",
       " ('KNeighborsClassifier',\n",
       "  {'Estimator__n_neighbors': 10,\n",
       "   'Estimator__weights': 'distance',\n",
       "   'SelectKBest__k': 23})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_stats_search_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimator</th>\n",
       "      <th>K best features</th>\n",
       "      <th>Mean CV Loss</th>\n",
       "      <th>Mean CV F1</th>\n",
       "      <th>Mean CV Accuracy</th>\n",
       "      <th>Val Loss</th>\n",
       "      <th>Val Precision</th>\n",
       "      <th>Val Recall</th>\n",
       "      <th>Val F1</th>\n",
       "      <th>Val Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>1</td>\n",
       "      <td>0.658261</td>\n",
       "      <td>0.668523</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.659491</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>18</td>\n",
       "      <td>0.628524</td>\n",
       "      <td>0.681248</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.715672</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>23</td>\n",
       "      <td>0.585960</td>\n",
       "      <td>0.684317</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.685331</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>15</td>\n",
       "      <td>0.605659</td>\n",
       "      <td>0.673632</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.673681</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>23</td>\n",
       "      <td>0.649512</td>\n",
       "      <td>0.639259</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.624133</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Estimator K best features  Mean CV Loss  Mean CV F1  \\\n",
       "0          LogisticRegression               1      0.658261    0.668523   \n",
       "1                         SVC              18      0.628524    0.681248   \n",
       "2      RandomForestClassifier              23      0.585960    0.684317   \n",
       "3  GradientBoostingClassifier              15      0.605659    0.673632   \n",
       "4        KNeighborsClassifier              23      0.649512    0.639259   \n",
       "\n",
       "   Mean CV Accuracy  Val Loss  Val Precision  Val Recall    Val F1  \\\n",
       "0          0.650000  0.659491       0.636364    0.466667  0.538462   \n",
       "1          0.662500  0.715672       0.461538    0.400000  0.428571   \n",
       "2          0.687500  0.685331       0.666667    0.533333  0.592593   \n",
       "3          0.675000  0.673681       0.700000    0.466667  0.560000   \n",
       "4          0.604167  0.624133       0.588235    0.666667  0.625000   \n",
       "\n",
       "   Val Accuracy  \n",
       "0      0.600000  \n",
       "1      0.466667  \n",
       "2      0.633333  \n",
       "3      0.633333  \n",
       "4      0.600000  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_stats_search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Model\n",
    "* Extract user usage of named entities, and create a feature vector from counts of the different named entities\n",
    "* \"TakeLab at SemEval-2019 Task 4: Hyperpartisan News Detection\" paper used an NER counter feature to help classify hyperpartisan news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_ner_labels = [\"PERSON\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \n",
    "                    \"LANGUAGE\", \"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entities_count_array(user_tweets):\n",
    "    \"\"\" Extract the named entities from a users tweets, and return an array of counts for each entity \"\"\"\n",
    "    freq = dict.fromkeys(spacy_ner_labels, 0)\n",
    "    for tweet in user_tweets:\n",
    "        cleaned_tweet = clean_text(tweet, remove_digits=False, remove_tags=True)\n",
    "        tweet_ne = spacy_nlp(cleaned_tweet).ents\n",
    "        for entity in tweet_ne:\n",
    "            freq[entity.label_] += 1\n",
    "    \n",
    "    return np.asarray(list(freq.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract NER count arrays\n",
    "ner_stats_extractor = TweetStatsExtractor(extractors=[named_entities_count_array])\n",
    "\n",
    "tweet_ner_stats_train = ner_stats_extractor.transform(tweet_train_processed)\n",
    "tweet_ner_stats_val = ner_stats_extractor.transform(tweet_val_processed)\n",
    "tweet_ner_stats_test = ner_stats_extractor.transform(tweet_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE: 11.846261024475098\n",
      "NORP: 8.961944580078125\n",
      "PERSON: 7.15403938293457\n",
      "ORDINAL: 6.141171455383301\n",
      "LAW: 4.278820514678955\n",
      "TIME: 3.648329734802246\n",
      "QUANTITY: 2.4599478244781494\n",
      "FAC: 1.7793406248092651\n",
      "PERCENT: 1.004050612449646\n",
      "ORG: 0.6693935990333557\n",
      "GPE: 0.530554473400116\n",
      "MONEY: 0.5099083781242371\n",
      "PRODUCT: 0.4269425570964813\n",
      "LANGUAGE: 0.34846231341362\n",
      "EVENT: 0.1474524587392807\n",
      "LOC: 0.09280887246131897\n",
      "CARDINAL: 0.07029922306537628\n",
      "WORK_OF_ART: 0.015256257727742195\n"
     ]
    }
   ],
   "source": [
    "print_features_by_importance(tweet_ner_stats_train, label_train, spacy_ner_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimators: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [18:44<00:00, 225.00s/it]\n"
     ]
    }
   ],
   "source": [
    "ner_search_best_params, ner_search_df = grid_search(tweet_ner_stats_train, label_train, tweet_ner_stats_val, label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LogisticRegression',\n",
       "  {'Estimator__C': 1,\n",
       "   'Estimator__penalty': 'l2',\n",
       "   'Estimator__solver': 'liblinear',\n",
       "   'SelectKBest__k': 5}),\n",
       " ('SVC',\n",
       "  {'Estimator__C': 4,\n",
       "   'Estimator__kernel': 'linear',\n",
       "   'Estimator__probability': True,\n",
       "   'SelectKBest__k': 5}),\n",
       " ('RandomForestClassifier',\n",
       "  {'Estimator__criterion': 'entropy',\n",
       "   'Estimator__min_samples_leaf': 8,\n",
       "   'Estimator__n_estimators': 50,\n",
       "   'SelectKBest__k': 7}),\n",
       " ('GradientBoostingClassifier',\n",
       "  {'Estimator__learning_rate': 0.05,\n",
       "   'Estimator__loss': 'deviance',\n",
       "   'Estimator__min_samples_leaf': 16,\n",
       "   'Estimator__n_estimators': 50,\n",
       "   'SelectKBest__k': 18}),\n",
       " ('KNeighborsClassifier',\n",
       "  {'Estimator__n_neighbors': 10,\n",
       "   'Estimator__weights': 'distance',\n",
       "   'SelectKBest__k': 7})]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_search_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimator</th>\n",
       "      <th>K best features</th>\n",
       "      <th>Mean CV Loss</th>\n",
       "      <th>Mean CV F1</th>\n",
       "      <th>Mean CV Accuracy</th>\n",
       "      <th>Val Loss</th>\n",
       "      <th>Val Precision</th>\n",
       "      <th>Val Recall</th>\n",
       "      <th>Val F1</th>\n",
       "      <th>Val Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>5</td>\n",
       "      <td>0.635042</td>\n",
       "      <td>0.673763</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.686513</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>5</td>\n",
       "      <td>0.627190</td>\n",
       "      <td>0.712966</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.671410</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>7</td>\n",
       "      <td>0.609674</td>\n",
       "      <td>0.666060</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.588716</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>18</td>\n",
       "      <td>0.619002</td>\n",
       "      <td>0.694489</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.730410</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>7</td>\n",
       "      <td>0.665317</td>\n",
       "      <td>0.605433</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.689034</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Estimator K best features  Mean CV Loss  Mean CV F1  \\\n",
       "0          LogisticRegression               5      0.635042    0.673763   \n",
       "1                         SVC               5      0.627190    0.712966   \n",
       "2      RandomForestClassifier               7      0.609674    0.666060   \n",
       "3  GradientBoostingClassifier              18      0.619002    0.694489   \n",
       "4        KNeighborsClassifier               7      0.665317    0.605433   \n",
       "\n",
       "   Mean CV Accuracy  Val Loss  Val Precision  Val Recall    Val F1  \\\n",
       "0          0.662500  0.686513       0.611111    0.733333  0.666667   \n",
       "1          0.691667  0.671410       0.611111    0.733333  0.666667   \n",
       "2          0.662500  0.588716       0.666667    0.666667  0.666667   \n",
       "3          0.687500  0.730410       0.611111    0.733333  0.666667   \n",
       "4          0.600000  1.689034       0.666667    0.800000  0.727273   \n",
       "\n",
       "   Val Accuracy  \n",
       "0      0.633333  \n",
       "1      0.633333  \n",
       "2      0.666667  \n",
       "3      0.633333  \n",
       "4      0.700000  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Model\n",
    "* Calculate the sentiment of a users tweets, and create a feature vector of these scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_sentiment_scores(user_tweets):\n",
    "    \"\"\" Returns the average, standard deviation, and max/min sentiment scores of the user \"\"\"\n",
    "    tweet_polarity = np.asarray([analyzer.polarity_scores(tweet)['compound'] for tweet in user_tweets])\n",
    "    sent_mean = np.mean(tweet_polarity, axis=0)\n",
    "    sent_std_dev = std_dev(tweet_polarity, sent_mean)\n",
    "    sent_max = np.max(tweet_polarity, axis=0)\n",
    "    sent_min = np.min(tweet_polarity, axis=0)\n",
    "    \n",
    "    num_pos, num_neu, num_neg = 0, 0, 0\n",
    "    for score in tweet_polarity:\n",
    "        if score >= 0.05:\n",
    "            num_pos += 1\n",
    "        elif score <= -0.05:\n",
    "            num_neg += 1\n",
    "        else:\n",
    "            num_neu += 1\n",
    "    \n",
    "    return np.asarray([sent_mean, sent_std_dev, sent_max, sent_min, num_pos, num_neu, num_neg])\n",
    "\n",
    "def overall_sentiment(user_tweets):\n",
    "    \"\"\" Returns the overall sentiment when all of the users tweets have been concatenated \"\"\"\n",
    "    return analyzer.polarity_scores(\". \".join(user_tweets))['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract NER count arrays\n",
    "sent_stats_extractor = TweetStatsExtractor(extractors=[tweet_sentiment_scores, overall_sentiment])\n",
    "\n",
    "tweet_sent_stats_train = sent_stats_extractor.transform(tweet_train_processed)\n",
    "tweet_sent_stats_val = sent_stats_extractor.transform(tweet_val_processed)\n",
    "tweet_sent_stats_test = sent_stats_extractor.transform(tweet_test_processed)\n",
    "\n",
    "sent_feature_names = [\n",
    "    \"Average tweet sentiment\",\n",
    "    \"Standard deviation of tweet sentiments\",\n",
    "    \"Max tweet sentiment\",\n",
    "    \"Min tweet sentiment\",\n",
    "    \"Number of positive tweets\",\n",
    "    \"Number of neutral tweets\",\n",
    "    \"Number of negative tweets\",\n",
    "    \"Overall sentiment of the user\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tweet sentiment: 22.31810779054663\n",
      "Number of negative tweets: 20.75132396112868\n",
      "Number of positive tweets: 16.62440774999441\n",
      "Overall sentiment of the user: 10.797528909878283\n",
      "Min tweet sentiment: 8.016052371960042\n",
      "Max tweet sentiment: 5.402185360047616\n",
      "Standard deviation of tweet sentiments: 1.6901325046002547\n",
      "Number of neutral tweets: 0.16105216203506037\n"
     ]
    }
   ],
   "source": [
    "print_features_by_importance(tweet_sent_stats_train, label_train, sent_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimators: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [09:58<00:00, 119.69s/it]\n"
     ]
    }
   ],
   "source": [
    "sent_search_best_params, sent_search_df = grid_search(\n",
    "    tweet_sent_stats_train, label_train, tweet_sent_stats_val, label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LogisticRegression',\n",
       "  {'Estimator__C': 1,\n",
       "   'Estimator__penalty': 'l1',\n",
       "   'Estimator__solver': 'liblinear',\n",
       "   'SelectKBest__k': 2}),\n",
       " ('SVC',\n",
       "  {'Estimator__C': 128,\n",
       "   'Estimator__kernel': 'linear',\n",
       "   'Estimator__probability': True,\n",
       "   'SelectKBest__k': 2}),\n",
       " ('RandomForestClassifier',\n",
       "  {'Estimator__criterion': 'entropy',\n",
       "   'Estimator__min_samples_leaf': 16,\n",
       "   'Estimator__n_estimators': 200,\n",
       "   'SelectKBest__k': 8}),\n",
       " ('GradientBoostingClassifier',\n",
       "  {'Estimator__learning_rate': 0.01,\n",
       "   'Estimator__loss': 'exponential',\n",
       "   'Estimator__min_samples_leaf': 14,\n",
       "   'Estimator__n_estimators': 50,\n",
       "   'SelectKBest__k': 7}),\n",
       " ('KNeighborsClassifier',\n",
       "  {'Estimator__n_neighbors': 10,\n",
       "   'Estimator__weights': 'uniform',\n",
       "   'SelectKBest__k': 1})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_search_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimator</th>\n",
       "      <th>K best features</th>\n",
       "      <th>Mean CV Loss</th>\n",
       "      <th>Mean CV F1</th>\n",
       "      <th>Mean CV Accuracy</th>\n",
       "      <th>Val Loss</th>\n",
       "      <th>Val Precision</th>\n",
       "      <th>Val Recall</th>\n",
       "      <th>Val F1</th>\n",
       "      <th>Val Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>2</td>\n",
       "      <td>0.658233</td>\n",
       "      <td>0.634904</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>0.599871</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>2</td>\n",
       "      <td>0.658014</td>\n",
       "      <td>0.643953</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.595261</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>8</td>\n",
       "      <td>0.658033</td>\n",
       "      <td>0.640914</td>\n",
       "      <td>0.629167</td>\n",
       "      <td>0.624175</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>7</td>\n",
       "      <td>0.670962</td>\n",
       "      <td>0.613900</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.686365</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700302</td>\n",
       "      <td>0.607157</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.610292</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Estimator K best features  Mean CV Loss  Mean CV F1  \\\n",
       "0          LogisticRegression               2      0.658233    0.634904   \n",
       "1                         SVC               2      0.658014    0.643953   \n",
       "2      RandomForestClassifier               8      0.658033    0.640914   \n",
       "3  GradientBoostingClassifier               7      0.670962    0.613900   \n",
       "4        KNeighborsClassifier               1      0.700302    0.607157   \n",
       "\n",
       "   Mean CV Accuracy  Val Loss  Val Precision  Val Recall    Val F1  \\\n",
       "0          0.620833  0.599871       0.687500    0.733333  0.709677   \n",
       "1          0.637500  0.595261       0.666667    0.666667  0.666667   \n",
       "2          0.629167  0.624175       0.769231    0.666667  0.714286   \n",
       "3          0.604167  0.686365       0.583333    0.466667  0.518519   \n",
       "4          0.600000  0.610292       0.666667    0.666667  0.666667   \n",
       "\n",
       "   Val Accuracy  \n",
       "0      0.700000  \n",
       "1      0.666667  \n",
       "2      0.733333  \n",
       "3      0.566667  \n",
       "4      0.666667  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Statistics Model\n",
    "* Concatenate all of the statistical features and input them into a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats data\n",
    "all_stats_extractor = TweetStatsExtractor(extractors=[\n",
    "    tag_counts,\n",
    "    retweet_ratio,\n",
    "    emojis_count,\n",
    "    syllables_to_words_ratios,\n",
    "    average_tweet_lengths,\n",
    "    word_type_to_token_ratio,\n",
    "    truncated_tweets,\n",
    "    punctuation_counts,\n",
    "    number_counts,\n",
    "    average_personal_pronouns,\n",
    "    char_to_words_ratio,\n",
    "    quote_counts,\n",
    "    capitalisation_counts,\n",
    "    named_entities_count_array,\n",
    "    tweet_sentiment_scores,\n",
    "    overall_sentiment,\n",
    "])\n",
    "tweet_all_stats_train = all_stats_extractor.transform(tweet_train_processed)\n",
    "tweet_all_stats_val = all_stats_extractor.transform(tweet_val_processed)\n",
    "tweet_all_stats_test = all_stats_extractor.transform(tweet_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimators: 100%|████████████████████████████████████████████████████████████████████| 5/5 [2:06:39<00:00, 1519.81s/it]\n"
     ]
    }
   ],
   "source": [
    "all_search_best_params, all_search_df = grid_search(\n",
    "    tweet_all_stats_train, label_train, tweet_all_stats_val, label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LogisticRegression',\n",
       "  {'Estimator__C': 1,\n",
       "   'Estimator__penalty': 'l1',\n",
       "   'Estimator__solver': 'liblinear',\n",
       "   'SelectKBest__k': 12}),\n",
       " ('SVC',\n",
       "  {'Estimator__C': 1,\n",
       "   'Estimator__kernel': 'rbf',\n",
       "   'Estimator__probability': True,\n",
       "   'SelectKBest__k': 44}),\n",
       " ('RandomForestClassifier',\n",
       "  {'Estimator__criterion': 'gini',\n",
       "   'Estimator__min_samples_leaf': 1,\n",
       "   'Estimator__n_estimators': 50,\n",
       "   'SelectKBest__k': 44}),\n",
       " ('GradientBoostingClassifier',\n",
       "  {'Estimator__learning_rate': 0.1,\n",
       "   'Estimator__loss': 'deviance',\n",
       "   'Estimator__min_samples_leaf': 16,\n",
       "   'Estimator__n_estimators': 25,\n",
       "   'SelectKBest__k': 38}),\n",
       " ('KNeighborsClassifier',\n",
       "  {'Estimator__n_neighbors': 10,\n",
       "   'Estimator__weights': 'distance',\n",
       "   'SelectKBest__k': 23})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_search_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimator</th>\n",
       "      <th>K best features</th>\n",
       "      <th>Mean CV Loss</th>\n",
       "      <th>Mean CV F1</th>\n",
       "      <th>Mean CV Accuracy</th>\n",
       "      <th>Val Loss</th>\n",
       "      <th>Val Precision</th>\n",
       "      <th>Val Recall</th>\n",
       "      <th>Val F1</th>\n",
       "      <th>Val Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>12</td>\n",
       "      <td>0.646358</td>\n",
       "      <td>0.690278</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.620576</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>44</td>\n",
       "      <td>0.594647</td>\n",
       "      <td>0.703312</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>0.619752</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>44</td>\n",
       "      <td>0.571723</td>\n",
       "      <td>0.691513</td>\n",
       "      <td>0.704167</td>\n",
       "      <td>0.632008</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>38</td>\n",
       "      <td>0.599312</td>\n",
       "      <td>0.686386</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.672012</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>23</td>\n",
       "      <td>0.621102</td>\n",
       "      <td>0.683979</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.678596</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Estimator K best features  Mean CV Loss  Mean CV F1  \\\n",
       "0          LogisticRegression              12      0.646358    0.690278   \n",
       "1                         SVC              44      0.594647    0.703312   \n",
       "2      RandomForestClassifier              44      0.571723    0.691513   \n",
       "3  GradientBoostingClassifier              38      0.599312    0.686386   \n",
       "4        KNeighborsClassifier              23      0.621102    0.683979   \n",
       "\n",
       "   Mean CV Accuracy  Val Loss  Val Precision  Val Recall    Val F1  \\\n",
       "0          0.675000  0.620576       0.714286    0.666667  0.689655   \n",
       "1          0.695833  0.619752       0.615385    0.533333  0.571429   \n",
       "2          0.704167  0.632008       0.727273    0.533333  0.615385   \n",
       "3          0.675000  0.672012       0.666667    0.666667  0.666667   \n",
       "4          0.633333  0.678596       0.533333    0.533333  0.533333   \n",
       "\n",
       "   Val Accuracy  \n",
       "0      0.700000  \n",
       "1      0.600000  \n",
       "2      0.666667  \n",
       "3      0.666667  \n",
       "4      0.533333  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining the Ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the best performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readability Model - RandomForestClassifier\n",
    "readability_model = Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('SelectKBest', SelectKBest()), \n",
    "    ('Estimator', RandomForestClassifier())\n",
    "])\n",
    "readability_model.set_params({\n",
    "    'Estimator__criterion': 'gini',\n",
    "    'Estimator__min_samples_leaf': 2,\n",
    "    'Estimator__n_estimators': 200,\n",
    "    'SelectKBest__k': 23,\n",
    "})\n",
    "\n",
    "readability_model.fit(tweet_read_stats_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER Model - RandomForestClassifier\n",
    "ner_model = Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('SelectKBest', SelectKBest()), \n",
    "    ('Estimator', RandomForestClassifier())\n",
    "])\n",
    "ner_model.set_params({\n",
    "    'Estimator__criterion': 'entropy',\n",
    "    'Estimator__min_samples_leaf': 8,\n",
    "    'Estimator__n_estimators': 50,\n",
    "    'SelectKBest__k': 7,\n",
    "})\n",
    "\n",
    "ner_model.fit(tweet_ner_stats_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Model - RandomForestClassifier\n",
    "sentiment_model = Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('SelectKBest', SelectKBest()), \n",
    "    ('Estimator', RandomForestClassifier())\n",
    "])\n",
    "sentiment_model.set_params({\n",
    "    'Estimator__criterion': 'entropy',\n",
    "    'Estimator__min_samples_leaf': 16,\n",
    "    'Estimator__n_estimators': 200,\n",
    "    'SelectKBest__k': 8,\n",
    "})\n",
    "\n",
    "sentiment_model.fit(tweet_sent_stats_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Statistics Model - \n",
    "all_stats_model = Pipeline([\n",
    "    ('SelectKBest', SelectKBest()), \n",
    "    ('Estimator', ?())\n",
    "])\n",
    "all_stats_model.set_params({\n",
    "\n",
    "})\n",
    "\n",
    "all_stats_model.fit(tweet_all_stats_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
