{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "preprocessing_path = 'C:\\\\Users\\\\joshh\\\\Desktop\\\\Uni\\\\Soton Uni - Yr 3\\\\COMP3200\\\\fake-news-profiling\\\\classifier\\\\preprocessing'\n",
    "if preprocessing_path not in sys.path:\n",
    "    sys.path.insert(1, preprocessing_path)\n",
    "\n",
    "notif_path = 'C:\\\\Users\\\\joshh\\\\Desktop\\\\Uni\\\\Soton Uni - Yr 3\\\\COMP3200\\\\fake-news-profiling\\\\classifier\\\\notifications'\n",
    "if notif_path not in sys.path:\n",
    "    sys.path.insert(1, notif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import ipynb.fs.full.parse_datasets as datasets\n",
    "import ipynb.fs.full.preprocessing as pp\n",
    "import ipynb.fs.full.bert_fake_news_classifier as bclf\n",
    "from ipynb.fs.full.notif_email import send_email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data, label_data = datasets.parse_dataset(\"datasets\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "tweet_preprocessor = pp.TweetPreprocessor(\n",
    "    preprocess_funcs = [\n",
    "        pp.tag_indicators,\n",
    "        pp.replace_xml_and_html,\n",
    "        pp.replace_emojis,\n",
    "        pp.remove_punctuation,\n",
    "        pp.replace_tags,\n",
    "        pp.remove_hashtag_chars,\n",
    "        pp.replace_accented_chars,\n",
    "        pp.tag_numbers,\n",
    "        pp.remove_stopwords,\n",
    "        pp.remove_extra_spacing,\n",
    "    ])\n",
    "tweet_preprocessor.preprocess(tweet_data)\n",
    "\n",
    "# Individual dataset\n",
    "tweet_data_individual = tweet_preprocessor.get_individual_tweets_dataset()\n",
    "\n",
    "# Split the data\n",
    "(tweet_train, label_train, \n",
    " tweet_val, label_val, \n",
    " tweet_test, label_test) = datasets.split_dataset(tweet_data_individual, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert encoder and tokenizer\n",
    "small_bert_url = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1\"\n",
    "bert_encoder_individual = hub.KerasLayer(\n",
    "    small_bert_url, \n",
    "    trainable=True,\n",
    ")\n",
    "bert_input_size_individual = 128\n",
    "\n",
    "individual_tokenizer = bclf.BertIndividualTweetTokenizer(bert_encoder_individual, bert_input_size_individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer data\n",
    "tweet_individual_train = individual_tokenizer.tokenize_input(tweet_train)\n",
    "label_individual_train = individual_tokenizer.tokenize_labels(label_train)\n",
    "tweet_individual_val = individual_tokenizer.tokenize_input(tweet_val)\n",
    "label_individual_val = individual_tokenizer.tokenize_labels(label_val)\n",
    "tweet_individual_test = individual_tokenizer.tokenize_input(tweet_test)\n",
    "label_individual_test = individual_tokenizer.tokenize_labels(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (210, 100, 3, 128), Label shape: (210,)\n"
     ]
    }
   ],
   "source": [
    "# Format data for joined BERT\n",
    "tweet_individual_train_joined = tf.convert_to_tensor([[\n",
    "    [tweet_individual_train['input_word_ids'][j*i],\n",
    "    tweet_individual_train['input_mask'][j*i],\n",
    "    tweet_individual_train['input_type_ids'][j*i],]\n",
    "    for j in range(100)\n",
    "] for i in range(int(len(tweet_individual_train['input_mask'])/100))])\n",
    "\n",
    "label_individual_train_joined = tf.convert_to_tensor([\n",
    "    label_individual_train.numpy()[i] \n",
    "    for i in range(0, len(tweet_individual_train['input_mask']), 100)\n",
    "])\n",
    "\n",
    "print(f\"Data shape: {tweet_individual_train_joined.shape}, Label shape: {label_individual_train_joined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joined training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "epochs = 4\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_model(encoder, input_size, dropout=0.1):\n",
    "    # Create BERT input layers\n",
    "    def input_layer(input_name):\n",
    "        return Input(shape=(input_size,), dtype=tf.int32, name=input_name)\n",
    "\n",
    "    inputs = {\n",
    "        'input_word_ids': input_layer(\"inputs/input_word_ids\"),\n",
    "        'input_mask': input_layer(\"inputs/input_mask\"),\n",
    "        'input_type_ids': input_layer(\"inputs/input_type_ids\"),\n",
    "    }\n",
    "\n",
    "    # BERT's pooled output\n",
    "    encoder_pooled_output = encoder(inputs)['pooled_output']\n",
    "    \n",
    "    # Dropout\n",
    "    dropout_out = tf.keras.layers.Dropout(dropout)(encoder_pooled_output)\n",
    "\n",
    "    # Dense layer output\n",
    "    dense_output = Dense(1, activation='sigmoid')(dropout_out)\n",
    "\n",
    "    # Create the Keras model and compile\n",
    "    return Model(inputs, dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT model\n",
    "bert_model = create_bert_model(bert_encoder_individual, bert_input_size_individual)\n",
    "bert_optimizer = Adam(learning_rate=learning_rate)\n",
    "bert_loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier model\n",
    "def create_clf_model():\n",
    "    input_layer = Input(shape=(100,))\n",
    "    dense_out = Dense(1, activation=\"sigmoid\")(input_layer)\n",
    "    return tf.keras.Model(inputs=input_layer, outputs=dense_out)\n",
    "\n",
    "clf_model = create_clf_model()\n",
    "clf_optimizer = Adam(lr=1e-5)\n",
    "clf_loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "clf_train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "clf_val_acc_metric = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 100, 3, 128), (None,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((tweet_individual_train_joined, label_individual_train_joined))\n",
    "train_dataset = train_dataset.batch(batch_size) # users per batch\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "Used: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard callback for logging loss\n",
    "model_name = f\"batch_size{batch_size}-epochs{epochs}-lr{learning_rate}\"\n",
    "model_path = \"training/bert_individual_joined/\"\n",
    "log_dir = model_path + \"logs/\" + model_name\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "# Checkpoint to save model\n",
    "bert_checkpoint_path = model_path + model_name + \"/bert/cp.ckpt\"\n",
    "clf_checkpoint_path = model_path + model_name + \"/clf/cp.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_bert_batch(tweets_batch):\n",
    "    res = {\n",
    "        'input_word_ids': tweets_batch[:, 0],\n",
    "        'input_mask': tweets_batch[:, 1],\n",
    "        'input_type_ids': tweets_batch[:, 2],\n",
    "    }\n",
    "    return res\n",
    "\n",
    "def to_tweet_batches(x_train, num_tweets, tweet_batch_size):\n",
    "    return [\n",
    "        tweets_to_bert_batch(x_train[i:i+tweet_batch_size]) \n",
    "        for i in range(0, num_tweets, tweet_batch_size)\n",
    "    ]\n",
    "\n",
    "def train_step(x_batch_train, y_batch_train, num_tweets=100, tweet_batch_size=10):\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as bert_tape, tf.GradientTape(watch_accessed_variables=False) as clf_tape:\n",
    "        bert_tape.watch(bert_model.variables)\n",
    "        bert_tape.watch(clf_model.variables)\n",
    "        clf_tape.watch(clf_model.variables)\n",
    "        \n",
    "        clf_batch_logits = []\n",
    "\n",
    "        # For each user in the batch\n",
    "        for x_user_train in x_batch_train:\n",
    "\n",
    "            # Predict tweet batches using BERT\n",
    "            bert_user_logits = tf.convert_to_tensor([])\n",
    "\n",
    "            for tweet_batch in to_tweet_batches(x_user_train, num_tweets, tweet_batch_size):\n",
    "                bert_batch_logits = tf.reshape(\n",
    "                    bert_model(tweet_batch, training=True), \n",
    "                    shape=(-1,),\n",
    "                )\n",
    "                bert_user_logits = tf.concat(\n",
    "                    (bert_user_logits, bert_batch_logits), \n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "            # Predict using Classifier\n",
    "            clf_inputs = tf.reshape(bert_user_logits, shape=(1, -1))\n",
    "            clf_logits = clf_model(clf_inputs, training=True)\n",
    "            clf_batch_logits.append(clf_logits)\n",
    "\n",
    "        # Take the loss of entire batch\n",
    "        clf_batch_logits_concat = tf.concat((clf_batch_logits), axis=0)\n",
    "\n",
    "        y_batch_train = tf.reshape(y_batch_train, shape=(-1, 1))\n",
    "        clf_batch_loss = clf_loss_fn(y_batch_train, clf_batch_logits_concat)\n",
    "\n",
    "    # Update gradients after batch\n",
    "    bert_grads = bert_tape.gradient(clf_batch_loss, bert_model.trainable_weights)\n",
    "    bert_optimizer.apply_gradients(zip(bert_grads, bert_model.trainable_weights))\n",
    "\n",
    "    clf_grads = clf_tape.gradient(clf_batch_loss, clf_model.trainable_weights)\n",
    "    clf_optimizer.apply_gradients(zip(clf_grads, clf_model.trainable_weights))\n",
    "\n",
    "    # Update training metric\n",
    "    clf_train_acc_metric.update_state(y_batch_train, clf_batch_logits_concat)\n",
    "    \n",
    "    # Return batch loss\n",
    "    return clf_batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-37629edc957e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mtotal_accuracy\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[1;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'cpu:0'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"\\nEpoch {epoch}/{epochs}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "total_loss = []\n",
    "total_accuracy = []\n",
    "\n",
    "with tf.device('cpu:0'):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        epoch_loss = []\n",
    "\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            batch_loss = train_step(x_batch_train, y_batch_train)\n",
    "            epoch_loss.append(batch_loss)\n",
    "            print(f\"> Batch {step}: loss={batch_loss}, accuracy={clf_train_acc_metric.result()}\")\n",
    "        \n",
    "        epoch_accuracy = clf_train_acc_metric.result()\n",
    "        print(\"Training acc over epoch: %.4f\" % (float(epoch_accuracy),))\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        clf_train_acc_metric.reset_states()\n",
    "        total_loss.append((epoch, epoch_loss))\n",
    "        total_accuracy.append((epoch, epoch_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "bert_model.save_weights(bert_checkpoint_path)\n",
    "clf_model.save_weights(clf_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((total_loss, total_accuracy), label=(\"Loss\", \"Accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}